<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Wei.Cao</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-03-07T07:28:34.051Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Wei.Cao</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>工作流调度器Azkaban</title>
    <link href="http://yoursite.com/2018/03/07/%E5%B7%A5%E4%BD%9C%E6%B5%81%E8%B0%83%E5%BA%A6%E5%99%A8azkaban/"/>
    <id>http://yoursite.com/2018/03/07/工作流调度器azkaban/</id>
    <published>2018-03-07T03:16:28.000Z</published>
    <updated>2018-03-07T07:28:34.051Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="为什么需要工作流调度系统"><a href="#为什么需要工作流调度系统" class="headerlink" title="为什么需要工作流调度系统"></a>为什么需要工作流调度系统</h3><ul><li>一个完整的数据分析系统通常都是由大量任务单元组成：<ul><li>shell脚本程序，java程序，mapreduce程序、hive脚本等</li></ul></li><li>各任务单元之间存在时间先后及前后依赖关系</li><li>为了很好地组织起这样的复杂执行计划，需要一个工作流调度系统来调度执行；</li><li>例如，我们可能有这样一个需求，某个业务系统每天产生20G原始数据，我们每天都要对其进行处理，处理步骤如下所示：<ol><li>通过Hadoop先将原始数据同步到HDFS上；</li><li>借助MapReduce计算框架对原始数据进行转换，生成的数据以分区表的形式存储到多张Hive表中；</li><li>需要对Hive中多个表的数据进行JOIN处理，得到一个明细数据Hive大表；</li><li>将明细数据进行复杂的统计分析，得到结果报表信息；</li><li>需要将统计分析得到的结果数据同步到业务系统中，供业务调用使用。</li></ol></li></ul><h3 id="工作流调度实现方式"><a href="#工作流调度实现方式" class="headerlink" title="工作流调度实现方式"></a>工作流调度实现方式</h3><ul><li>简单的任务调度：直接使用linux的crontab来定义；</li><li>复杂的任务调度：开发调度平台</li><li>或使用现成的开源调度系统，比如ooize、azkaban等</li></ul><h3 id="常见工作流调度系统"><a href="#常见工作流调度系统" class="headerlink" title="常见工作流调度系统"></a>常见工作流调度系统</h3><ul><li>市面上目前有许多工作流调度器</li><li>在hadoop领域，常见的工作流调度器有Oozie, Azkaban,Cascading,Hamake等</li></ul><h3 id="各种调度工具特性对比"><a href="#各种调度工具特性对比" class="headerlink" title="各种调度工具特性对比"></a>各种调度工具特性对比</h3><ul><li><p>下面的表格对上述四种hadoop工作流调度器的关键特性进行了比较，尽管这些工作流调度器能够解决的需求场景基本一致，但在设计理念，目标用户，应用场景等方面还是存在显著的区别，在做技术选型的时候，可以提供参考</p><p>| 特性               | Hamake               | Oozie                | Azkaban                        | Cascading |<br>| —————— | ——————– | ——————– | —————————— | ——— |<br>| 工作流描述语言     | XML                  | XML (xPDL based)     | text file with key/value pairs | Java API  |<br>| 依赖机制           | data-driven          | explicit             | explicit                       | explicit  |<br>| 是否要web容器      | No                   | Yes                  | Yes                            | No        |<br>| 进度跟踪           | console/log          | messages    web page | web page                       | Java API  |<br>| Hadoop job调度支持 | no                   | yes                  | yes                            | yes       |<br>| 运行模式           | command line utility | daemon               | daemon                         | API       |<br>| Pig支持            | yes                  | yes                  | yes                            | yes       |<br>| 事件通知           | no                   | no                   | no                             | no        |<br>| 需要安装           | no                   | yes                  | yes                            | no        |<br>| 支持的hadoop版本   | 0.18+                | 0.20+                | currently unknown              | 0.18+     |<br>| 重试支持           | no                   | workflownode evel    | yes                            | yes       |<br>| 运行任意命令       | yes                  | yes                  | yes                            | yes       |<br>| Amazon EMR支持     | yes                  | no                   | currently unknown              | yes       |</p></li></ul><h3 id="Azkaban与Oozie对比"><a href="#Azkaban与Oozie对比" class="headerlink" title="Azkaban与Oozie对比"></a>Azkaban与Oozie对比</h3><ul><li>对市面上最流行的两种调度器，给出以下详细对比，以供技术选型参考。总体来说，ooize相比azkaban是一个重量级的任务调度系统，功能全面，但配置使用也更复杂。如果可以不在意某些功能的缺失，轻量级调度器azkaban是很不错的候选对象。</li><li>详情如下：</li><li>功能<ul><li>两者均可以调度mapreduce,pig,java,脚本工作流任务</li><li>两者均可以定时执行工作流任务</li></ul></li><li>工作流定义<ul><li>Azkaban使用Properties文件定义工作流</li><li>Oozie使用XML文件定义工作流</li></ul></li><li>工作流传参<ul><li>Azkaban支持直接传参，例如${input}</li><li>Oozie支持参数和EL表达式，例如${fs:dirSize(myInputDir)}</li></ul></li><li>定时执行<ul><li>Azkaban的定时执行任务是基于时间的</li><li>Oozie的定时执行任务基于时间和输入数据</li></ul></li><li>资源管理<ul><li>Azkaban有较严格的权限控制，如用户对工作流进行读/写/执行等操作</li><li>Oozie暂无严格的权限控制</li></ul></li><li>工作流执行<ul><li>Azkaban有两种运行模式，分别是solo server mode(executor server和web server部署在同一台节点)和multi server mode(executor server和web server可以部署在不同节点)</li><li>Oozie作为工作流服务器运行，支持多用户和多工作流</li></ul></li><li>工作流管理<ul><li>Azkaban支持浏览器以及ajax方式操作工作流</li><li>Oozie支持命令行、HTTP REST、Java API、浏览器操作工作流</li></ul></li></ul><h2 id="Azkaban介绍"><a href="#Azkaban介绍" class="headerlink" title="Azkaban介绍"></a>Azkaban介绍</h2><ul><li>Azkaban是由Linkedin开源的一个批量工作流任务调度器。用于在一个工作流内以一个特定的顺序运行一组工作和流程。Azkaban定义了一种KV文件格式来建立任务之间的依赖关系，并提供一个易于使用的web用户界面维护和跟踪你的工作流。</li><li>它有如下功能特点：<ul><li>Web用户界面</li><li>方便上传工作流</li><li>方便设置任务之间的关系</li><li>调度工作流</li><li>认证/授权(权限的工作)</li><li>能够杀死并重新启动工作流</li><li>模块化和可插拔的插件机制</li><li>项目工作区</li><li>工作流和任务的日志记录和审计</li></ul></li></ul><h2 id="Azkaban安装部署"><a href="#Azkaban安装部署" class="headerlink" title="Azkaban安装部署"></a>Azkaban安装部署</h2><ul><li><p>准备工作</p><ul><li>Azkaban Web服务器</li></ul></li></ul><ul><li>azkaban-web-server-2.5.0.tar.gz</li></ul><ul><li>Azkaban执行服务器 </li></ul><ul><li>azkaban-executor-server-2.5.0.tar.gz</li></ul><ul><li>MySQL</li></ul><ul><li>目前azkaban只支持 mysql,需安装mysql服务器,本文档中默认已安装好mysql服务器,并建立了 root用户,密码 root.</li></ul><ul><li>下载地址:<a href="http://azkaban.github.io/downloads.html" target="_blank" rel="noopener">http://azkaban.github.io/downloads.html</a></li></ul><ul><li>安装</li></ul><ul><li>将安装文件上传到集群,最好上传到安装 hive、sqoop的机器上,方便命令的执行</li></ul><ul><li>在当前用户目录下新建 azkabantools目录,用于存放源安装文件.新建azkaban目录,用于存放azkaban运行程序</li></ul><p>azkaban web服务器安装</p><ul><li>解压azkaban-web-server-2.5.0.tar.gz</li></ul><ul><li>命令: <code>tar –zxvf azkaban-web-server-2.5.0.tar.gz</code></li></ul><ul><li>将解压后的azkaban-web-server-2.5.0 移动到 azkaban目录中,并重新命名 webserver</li></ul><ul><li>命令: <code>mv azkaban-web-server-2.5.0 ../azkaban</code></li><li><code>cd ../azkaban</code></li><li><code>mv azkaban-web-server-2.5.0  server</code></li></ul><p>azkaban 执行服器安装</p><ul><li>解压azkaban-executor-server-2.5.0.tar.gz</li><li>命令:<code>tar –zxvf azkaban-executor-server-2.5.0.tar.gz</code></li><li>将解压后的azkaban-executor-server-2.5.0 移动到 azkaban目录中,并重新命名 executor</li><li>命令:<code>mv azkaban-executor-server-2.5.0  ../azkaban</code></li><li><code>cd ../azkaban</code></li><li><code>mv azkaban-executor-server-2.5.0  executor</code></li></ul><p>azkaban脚本导入</p><ul><li>解压: azkaban-sql-script-2.5.0.tar.gz</li><li>命令:<code>tar –zxvf azkaban-sql-script-2.5.0.tar.gz</code></li><li>将解压后的mysql 脚本,导入到mysql中:</li><li>进入mysql</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt; create database azkaban;</span><br><span class="line">mysql&gt; use azkaban;</span><br><span class="line">Database changed</span><br><span class="line">mysql&gt; source /home/hadoop/azkaban-2.5.0/create-all-sql-2.5.0.sql;</span><br></pre></td></tr></table></figure><p>创建SSL配置</p><ul><li>参考地址: <a href="http://docs.codehaus.org/display/JETTY/How+to+configure+SSL" target="_blank" rel="noopener">http://docs.codehaus.org/display/JETTY/How+to+configure+SSL</a><ul><li>命令: <code>keytool -keystore keystore -alias jetty -genkey -keyalg RSA</code></li><li>运行此命令后,会提示输入当前生成 keystor的密码及相应信息,输入的密码请劳记,信息如下:</li><li>输入keystore密码： </li><li>再次输入新密码:</li><li>您的名字与姓氏是什么？</li><li>[Unknown]： </li><li>您的组织单位名称是什么？</li><li>[Unknown]： </li><li>您的组织名称是什么？</li><li>[Unknown]： </li><li>您所在的城市或区域名称是什么？</li><li>[Unknown]： </li><li>您所在的州或省份名称是什么？</li><li>[Unknown]： </li><li>该单位的两字母国家代码是什么</li><li>[Unknown]：  CN</li><li>CN=Unknown, OU=Unknown, O=Unknown, L=Unknown, ST=Unknown, C=CN 正确吗？</li><li>[否]：  y</li><li>输入\<jetty>的主密码<ul><li>（如果和 keystore 密码相同，按回车）：</li></ul></jetty></li><li>再次输入新密码:</li><li>完成上述工作后,将在当前目录生成 keystore 证书文件,将keystore 考贝到 azkaban web服务器根目录中.<ul><li><code>如:cp keystore azkaban/server</code></li></ul></li></ul></li></ul><h3 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h3><ul><li>注：先配置好服务器节点上的时区</li></ul><ol><li>先生成时区配置文件Asia/Shanghai，用交互式命令 tzselect 即可</li><li>拷贝该时区文件，覆盖系统本地时区配置<ul><li><code>cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</code>  </li></ul></li></ol><ul><li><p>azkaban web服务器配置</p><ol><li>进入azkaban web服务器安装目录 conf目录</li><li>修改azkaban.properties文件</li><li>命令<code>vi azkaban.properties</code></li></ol><ul><li>内容说明如下:</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">#Azkaban Personalization Settings</span><br><span class="line">azkaban.name=Test                           #服务器UI名称,用于服务器上方显示的名字</span><br><span class="line">azkaban.label=My Local Azkaban              #描述</span><br><span class="line">azkaban.color=#FF3601                       #UI颜色</span><br><span class="line">azkaban.default.servlet.path=/index         #</span><br><span class="line">web.resource.dir=web/                       #默认根web目录</span><br><span class="line">default.timezone.id=Asia/Shanghai           #默认时区,已改为亚洲/上海 默认为美国</span><br><span class="line"> </span><br><span class="line">#Azkaban UserManager class</span><br><span class="line">user.manager.class=azkaban.user.XmlUserManager   #用户权限管理默认类</span><br><span class="line">user.manager.xml.file=conf/azkaban-users.xml     #用户配置,具体配置参加下文</span><br><span class="line"> </span><br><span class="line">#Loader for projects</span><br><span class="line">executor.global.properties=conf/global.properties  # global配置文件所在位置</span><br><span class="line">azkaban.project.dir=projects                       #</span><br><span class="line"> </span><br><span class="line">database.type=mysql                                #数据库类型</span><br><span class="line">mysql.port=3306                                    #端口号</span><br><span class="line">mysql.host=localhost                               #数据库连接IP</span><br><span class="line">mysql.database=azkaban                             #数据库实例名</span><br><span class="line">mysql.user=root                                    #数据库用户名</span><br><span class="line">mysql.password=root                                #数据库密码</span><br><span class="line">mysql.numconnections=100                           #最大连接数</span><br><span class="line"> </span><br><span class="line"># Velocity dev mode</span><br><span class="line">velocity.dev.mode=false</span><br><span class="line"># Jetty服务器属性.</span><br><span class="line">jetty.maxThreads=25                                #最大线程数</span><br><span class="line">jetty.ssl.port=8443                                #Jetty SSL端口</span><br><span class="line">jetty.port=8081                                    #Jetty端口</span><br><span class="line">jetty.keystore=keystore                            #SSL文件名</span><br><span class="line">jetty.password=123456                              #SSL文件密码</span><br><span class="line">jetty.keypassword=123456                           #Jetty主密码 与 keystore文件相同</span><br><span class="line">jetty.truststore=keystore                          #SSL文件名</span><br><span class="line">jetty.trustpassword=123456                         #SSL文件密码</span><br><span class="line"> </span><br><span class="line"># 执行服务器属性</span><br><span class="line">executor.port=12321                                #执行服务器端口</span><br><span class="line"># 邮件设置</span><br><span class="line">mail.sender=xxxxxxxx@163.com                       #发送邮箱</span><br><span class="line">mail.host=smtp.163.com                             #发送邮箱smtp地址</span><br><span class="line">mail.user=xxxxxxxx                                 #发送邮件时显示的名称</span><br><span class="line">mail.password=**********                           #邮箱密码</span><br><span class="line">job.failure.email=xxxxxxxx@163.com                 #任务失败时发送邮件的地址</span><br><span class="line">job.success.email=xxxxxxxx@163.com                 #任务成功时发送邮件的地址</span><br><span class="line">lockdown.create.projects=false                     #</span><br><span class="line">cache.directory=cache                              #缓存目录</span><br></pre></td></tr></table></figure></li><li><p>azkaban 执行服务器executor配置</p><ul><li>进入执行服务器安装目录conf,修改azkaban.properties</li><li><code>vi azkaban.properties</code></li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#Azkaban</span><br><span class="line">default.timezone.id=Asia/Shanghai                    #时区</span><br><span class="line"> </span><br><span class="line"># Azkaban JobTypes 插件配置</span><br><span class="line">azkaban.jobtype.plugin.dir=plugins/jobtypes          #jobtype 插件所在位置</span><br><span class="line"> </span><br><span class="line">#Loader for projects</span><br><span class="line">executor.global.properties=conf/global.properties</span><br><span class="line">azkaban.project.dir=projects</span><br><span class="line"> </span><br><span class="line">#数据库设置</span><br><span class="line">database.type=mysql                                  #数据库类型(目前只支持mysql)</span><br><span class="line">mysql.port=3306                                      #数据库端口号</span><br><span class="line">mysql.host=192.168.20.200                            #数据库IP地址</span><br><span class="line">mysql.database=azkaban                               #数据库实例名</span><br><span class="line">mysql.user=root                                      #数据库用户名</span><br><span class="line">mysql.password=root                                  #数据库密码</span><br><span class="line">mysql.numconnections=100                             #最大连接数</span><br><span class="line"> </span><br><span class="line"># 执行服务器配置</span><br><span class="line">executor.maxThreads=50                               #最大线程数</span><br><span class="line">executor.port=12321                                  #端口号(如修改,请与web服务中一致)</span><br><span class="line">executor.flow.threads=30</span><br></pre></td></tr></table></figure><ul><li><p>azkaban 执行服务器executor配置</p><ul><li>进入执行服务器安装目录conf,修改azkaban.properties</li><li>vi azkaban.properties</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">#Azkaban</span><br><span class="line">default.timezone.id=Asia/Shanghai                    #时区</span><br><span class="line"> </span><br><span class="line"># Azkaban JobTypes 插件配置</span><br><span class="line">azkaban.jobtype.plugin.dir=plugins/jobtypes          #jobtype 插件所在位置</span><br><span class="line"> </span><br><span class="line">#Loader for projects</span><br><span class="line">executor.global.properties=conf/global.properties</span><br><span class="line">azkaban.project.dir=projects</span><br><span class="line"> </span><br><span class="line">#数据库设置</span><br><span class="line">database.type=mysql                                  #数据库类型(目前只支持mysql)</span><br><span class="line">mysql.port=3306                                      #数据库端口号</span><br><span class="line">mysql.host=192.168.20.200                            #数据库IP地址</span><br><span class="line">mysql.database=azkaban                               #数据库实例名</span><br><span class="line">mysql.user=root                                      #数据库用户名</span><br><span class="line">mysql.password=root                                  #数据库密码</span><br><span class="line">mysql.numconnections=100                             #最大连接数</span><br><span class="line"> </span><br><span class="line"># 执行服务器配置</span><br><span class="line">executor.maxThreads=50                               #最大线程数</span><br><span class="line">executor.port=12321                                  #端口号(如修改,请与web服务中一致)</span><br><span class="line">executor.flow.threads=30                             #线程数</span><br></pre></td></tr></table></figure></li><li><p>用户配置</p><ul><li>进入azkaban web服务器conf目录,修改azkaban-users.xml</li><li><code>vi azkaban-users.xml</code> 增加 管理员用户</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">azkaban-users</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">user</span> <span class="attr">username</span>=<span class="string">"azkaban"</span> <span class="attr">password</span>=<span class="string">"azkaban"</span> <span class="attr">roles</span>=<span class="string">"admin"</span> <span class="attr">groups</span>=<span class="string">"azkaban"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">user</span> <span class="attr">username</span>=<span class="string">"metrics"</span> <span class="attr">password</span>=<span class="string">"metrics"</span> <span class="attr">roles</span>=<span class="string">"metrics"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">user</span> <span class="attr">username</span>=<span class="string">"admin"</span> <span class="attr">password</span>=<span class="string">"admin"</span> <span class="attr">roles</span>=<span class="string">"admin,metrics"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">role</span> <span class="attr">name</span>=<span class="string">"admin"</span> <span class="attr">permissions</span>=<span class="string">"ADMIN"</span> /&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">role</span> <span class="attr">name</span>=<span class="string">"metrics"</span> <span class="attr">permissions</span>=<span class="string">"METRICS"</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">azkaban-users</span>&gt;</span></span><br></pre></td></tr></table></figure><p>​</p></li></ul><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><p>####web服务器</p><ul><li>在azkaban web服务器目录下执行启动命令<ul><li><code>bin/azkaban-web-start.sh</code></li><li>注:在web服务器根目录运行</li></ul></li><li>或者启动到后台<ul><li><code>nohup  bin/azkaban-web-start.sh  1&gt;/tmp/azstd.out  2&gt;/tmp/azerr.out &amp;</code></li></ul></li></ul><h4 id="执行服务器"><a href="#执行服务器" class="headerlink" title="执行服务器"></a>执行服务器</h4><ul><li>在执行服务器目录下执行启动命令<ul><li><code>bin/azkaban-executor-start.sh</code></li><li>注:只能要执行服务器根目录运行</li></ul></li><li>启动完成后,在浏览器(建议使用谷歌浏览器)中输入https://服务器IP地址:8443 ,即可访问azkaban服务了.在登录中输入刚才新的户用名及密码,点击 login.</li></ul><h2 id="Azkaban实战"><a href="#Azkaban实战" class="headerlink" title="Azkaban实战"></a>Azkaban实战</h2><ul><li>Azkaba内置的任务类型支持command、java</li></ul><h3 id="Command类型单一job示例"><a href="#Command类型单一job示例" class="headerlink" title="Command类型单一job示例"></a>Command类型单一job示例</h3><ol><li><p>创建job描述文件</p><ul><li><p><code>vi command.job</code></p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#command.job</span><br><span class="line">type=command                                                    </span><br><span class="line">command=echo &apos;hello&apos;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>将job资源文件打包成zip文件</p><ul><li><code>zip command.job</code></li></ul></li><li><p>通过azkaban的web管理平台创建project并上传job压缩包</p><ul><li>首先创建project</li><li><img src="http://ou9crezlk.bkt.clouddn.com/blog/180307/0mme9haLAi.png?imageslim" alt="mark"></li><li>上传zip包</li><li><img src="http://ou9crezlk.bkt.clouddn.com/blog/180307/LgGjK4L25J.png?imageslim" alt="mark"></li></ul></li><li><p>启动执行该job</p><p><img src="http://ou9crezlk.bkt.clouddn.com/blog/180307/ljfDakbmBF.png?imageslim" alt="mark"></p></li></ol><h3 id="Command类型多job工作流flow"><a href="#Command类型多job工作流flow" class="headerlink" title="Command类型多job工作流flow"></a>Command类型多job工作流flow</h3><ol><li><p>创建有依赖关系的多个job描述</p><ul><li>第一个job：foo.job</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#foo.job</span><br><span class="line">type=command</span><br><span class="line">command=echo foo</span><br></pre></td></tr></table></figure><ul><li><p>第二个job：bar.job依赖foo.job</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#bar.job</span><br><span class="line">type=command</span><br><span class="line">dependencies=foo</span><br><span class="line">command=echo bar</span><br></pre></td></tr></table></figure></li></ul></li><li><p>将所有job资源文件打到一个zip包中</p></li><li><p>在azkaban的web管理界面创建工程并上传zip包</p></li><li><p>启动工作流flow</p></li></ol><h3 id="HDFS操作任务"><a href="#HDFS操作任务" class="headerlink" title="HDFS操作任务"></a>HDFS操作任务</h3><ol><li><p>创建job描述文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#fs.job</span><br><span class="line">type=command</span><br><span class="line">command=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop fs -mkdir /azaz</span><br></pre></td></tr></table></figure></li><li><p>将job资源文件打包成zip文件</p></li><li><p>通过azkaban的web管理平台创建project并上传job压缩包</p></li><li><p>启动执行该job</p></li></ol><h3 id="MAPREDUCE任务"><a href="#MAPREDUCE任务" class="headerlink" title="MAPREDUCE任务"></a>MAPREDUCE任务</h3><ul><li>Mr任务依然可以使用command的job类型来执行</li></ul><ol><li><p>创建job描述文件，及mr程序jar包（示例中直接使用hadoop自带的example jar）</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#mrwc.job</span><br><span class="line">type=command</span><br><span class="line">command=/home/hadoop/apps/hadoop-2.6.1/bin/hadoop  jar hadoop-mapreduce-examples-2.6.1.jar wordcount /wordcount/input /wordcount/azout</span><br></pre></td></tr></table></figure></li><li><p>将所有job资源文件打到一个zip包中</p></li><li><p>在azkaban的web管理界面创建工程并上传zip包</p></li><li><p>启动job</p></li></ol><h3 id="HIVE脚本任务"><a href="#HIVE脚本任务" class="headerlink" title="HIVE脚本任务"></a>HIVE脚本任务</h3><ol><li><p>创建job描述文件和hive脚本</p><ul><li><p>Hive脚本： test.sql</p></li><li><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">use</span> <span class="keyword">default</span>;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> aztest;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> aztest(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> inpath <span class="string">'/aztest/hiveinput'</span> <span class="keyword">into</span> <span class="keyword">table</span> aztest;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> azres <span class="keyword">as</span> <span class="keyword">select</span> * <span class="keyword">from</span> aztest;</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">directory</span> <span class="string">'/aztest/hiveoutput'</span> <span class="keyword">select</span> <span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">from</span> aztest;</span><br></pre></td></tr></table></figure></li><li><p>Job描述文件：hivef.job</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hivef.job</span><br><span class="line">type=command</span><br><span class="line">command=/home/hadoop/apps/hive/bin/hive -f &apos;test.sql&apos;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>将所有job资源文件打到一个zip包中</p></li><li><p>在azkaban的web管理界面创建工程并上传zip包</p></li><li><p>启动job</p></li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;h3 id=&quot;为什么需要工作流调度系统&quot;&gt;&lt;a href=&quot;#为什么需要工作流调度系统&quot; class=&quot;headerlink&quot; title=&quot;为
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://yoursite.com/tags/Hadoop/"/>
    
      <category term="Java" scheme="http://yoursite.com/tags/Java/"/>
    
      <category term="Azkaban" scheme="http://yoursite.com/tags/Azkaban/"/>
    
  </entry>
  
  <entry>
    <title>日志采集框架Flume</title>
    <link href="http://yoursite.com/2018/03/07/%E6%97%A5%E5%BF%97%E9%87%87%E9%9B%86%E6%A1%86%E6%9E%B6Flume/"/>
    <id>http://yoursite.com/2018/03/07/日志采集框架Flume/</id>
    <published>2018-03-07T03:16:28.000Z</published>
    <updated>2018-03-07T03:38:56.234Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Flume介绍"><a href="#Flume介绍" class="headerlink" title="Flume介绍"></a>Flume介绍</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><ul><li>Flume是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。</li></ul><ul><li>Flume可以采集文件，socket数据包等各种形式源数据，又可以将采集到的数据输出到HDFS、hbase、hive、kafka等众多外部存储系统中</li></ul><ul><li>一般的采集需求，通过对flume的简单配置即可实现</li></ul><ul><li>Flume针对特殊场景也具备良好的自定义扩展能力，因此，flume可以适用于大部分的日常数据采集场景</li></ul><h3 id="运行机制"><a href="#运行机制" class="headerlink" title="运行机制"></a>运行机制</h3><ol><li>Flume分布式系统中最核心的角色是agent，flume采集系统就是由一个个agent所连接起来形成</li><li>每一个agent相当于一个数据传递员，内部有三个组件：<ol><li>Source：采集源，用于跟数据源对接，以获取数据</li><li>Sink：下沉地，采集数据的传送目的，用于往下一级agent传递数据或者往最终存储系统传递数</li><li>Channel：angent内部的数据传输通道，用于从source将数据传递到sink</li></ol></li></ol><h3 id="Flume采集系统结构图"><a href="#Flume采集系统结构图" class="headerlink" title="Flume采集系统结构图"></a>Flume采集系统结构图</h3><ol><li><p>简单结构<br>单个agent采集数据</p><p><img src="http://ou9crezlk.bkt.clouddn.com/blog/180307/4i2gFHDlKe.png?imageslim" alt="mark"></p></li><li><p>复杂结构<br>多级agent之间串联</p><p><img src="http://ou9crezlk.bkt.clouddn.com/blog/180307/Kh0ahalF80.png?imageslim" alt="mark"></p></li></ol><h2 id="Flume实战案例"><a href="#Flume实战案例" class="headerlink" title="Flume实战案例"></a>Flume实战案例</h2><h3 id="Flume的安装部署"><a href="#Flume的安装部署" class="headerlink" title="Flume的安装部署"></a>Flume的安装部署</h3><ol><li>Flume的安装非常简单，只需要解压即可，当然，前提是已有hadoop环境</li></ol><p>上传安装包到数据源所在节点上<br>然后解压  tar -zxvf apache-flume-1.6.0-bin.tar.gz<br>然后进入flume的目录，修改conf下的flume-env.sh，在里面配置JAVA_HOME</p><ol><li>根据数据采集的需求配置采集方案，描述在配置文件中(文件名可任意自定义)</li></ol><ol><li>指定采集方案配置文件，在相应的节点上启动flume agent</li></ol><ul><li><p>先用一个最简单的例子来测试一下程序环境是否正常</p><ol><li>先在flume的conf目录下新建一个文件</li></ol><ul><li><code>vi   netcat-logger.conf</code></li></ul></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># 定义这个agent中各组件的名字</span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.channels = c1</span><br><span class="line"></span><br><span class="line"># 描述和配置source组件：r1</span><br><span class="line">a1.sources.r1.type = netcat</span><br><span class="line">a1.sources.r1.bind = localhost</span><br><span class="line">a1.sources.r1.port = 44444</span><br><span class="line"></span><br><span class="line"># 描述和配置sink组件：k1</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"># 描述和配置channel组件，此处使用是内存缓存的方式</span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line"># 描述和配置source  channel   sink之间的连接关系</span><br><span class="line">a1.sources.r1.channels = c1</span><br><span class="line">a1.sinks.k1.channel = c1</span><br></pre></td></tr></table></figure><ol><li>启动agent去采集数据</li></ol><p><code>bin/flume-ng agent -c conf -f conf/netcat-logger.conf -n a1  -Dflume.root.logger=INFO,console</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-c conf   指定flume自身的配置文件所在目录</span><br><span class="line">-f conf/netcat-logger.con  指定我们所描述的采集方案</span><br><span class="line">-n a1  指定我们这个agent的名字</span><br></pre></td></tr></table></figure><ol><li>测试<ul><li>先要往agent采集监听的端口上发送数据，让agent有数据可采随便在一个能跟agent节点联网的机器上</li><li><code>telnet anget-hostname  port</code></li><li><code>如：（telnet localhost 44444）</code> </li></ul></li></ol><h3 id="采集案例"><a href="#采集案例" class="headerlink" title="采集案例"></a>采集案例</h3><h4 id="采集目录到HDFS"><a href="#采集目录到HDFS" class="headerlink" title="采集目录到HDFS"></a>采集目录到HDFS</h4><ul><li>采集需求：某服务器的某特定目录下，会不断产生新的文件，每当有新文件出现，就需要把文件采集到HDFS中去</li><li>根据需求，首先定义以下3大要素<ul><li>采集源，即source——监控文件目录 :  spooldir</li><li>下沉目标，即sink——HDFS文件系统  :  hdfs sink</li><li>source和sink之间的传递通道——channel，可用file channel 也可以用内存channel</li></ul></li><li>配置文件编写：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">agent1.sources = source1</span><br><span class="line">agent1.sinks = sink1</span><br><span class="line">agent1.channels = channel1</span><br><span class="line"></span><br><span class="line"># 配置source组件</span><br><span class="line">agent1.sources.source1.type = spooldir</span><br><span class="line">agent1.sources.source1.spoolDir = /home/hadoop/logs/</span><br><span class="line">agent1.sources.source1.fileHeader = false</span><br><span class="line"></span><br><span class="line">#配置拦截器</span><br><span class="line">agent1.sources.source1.interceptors = i1</span><br><span class="line">agent1.sources.source1.interceptors.i1.type = host</span><br><span class="line">agent1.sources.source1.interceptors.i1.hostHeader = hostname</span><br><span class="line"></span><br><span class="line"># 配置sink组件</span><br><span class="line">agent1.sinks.sink1.type = hdfs</span><br><span class="line">agent1.sinks.sink1.hdfs.path =hdfs://hdp-node-01:9000/weblog/flume-collection/%y-%m-%d/%H-%M</span><br><span class="line">agent1.sinks.sink1.hdfs.filePrefix = access_log</span><br><span class="line">agent1.sinks.sink1.hdfs.maxOpenFiles = 5000</span><br><span class="line">agent1.sinks.sink1.hdfs.batchSize= 100</span><br><span class="line">agent1.sinks.sink1.hdfs.fileType = DataStream</span><br><span class="line">agent1.sinks.sink1.hdfs.writeFormat =Text</span><br><span class="line">agent1.sinks.sink1.hdfs.rollSize = 102400</span><br><span class="line">agent1.sinks.sink1.hdfs.rollCount = 1000000</span><br><span class="line">agent1.sinks.sink1.hdfs.rollInterval = 60</span><br><span class="line">#agent1.sinks.sink1.hdfs.round = true</span><br><span class="line">#agent1.sinks.sink1.hdfs.roundValue = 10</span><br><span class="line">#agent1.sinks.sink1.hdfs.roundUnit = minute</span><br><span class="line">agent1.sinks.sink1.hdfs.useLocalTimeStamp = true</span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">agent1.channels.channel1.type = memory</span><br><span class="line">agent1.channels.channel1.keep-alive = 120</span><br><span class="line">agent1.channels.channel1.capacity = 500000</span><br><span class="line">agent1.channels.channel1.transactionCapacity = 600</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">agent1.sources.source1.channels = channel1</span><br><span class="line">agent1.sinks.sink1.channel = channel1</span><br></pre></td></tr></table></figure><ul><li>Channel参数解释：<ul><li>capacity：默认该通道中最大的可以存储的event数量</li><li>trasactionCapacity：每次最大可以从source中拿到或者送到sink中的event数量</li><li>keep-alive：event添加到通道中或者移出的允许时间</li></ul></li></ul><h4 id="采集文件到HDFS"><a href="#采集文件到HDFS" class="headerlink" title="采集文件到HDFS"></a>采集文件到HDFS</h4><ul><li><p>采集需求：比如业务系统使用log4j生成的日志，日志内容不断增加，需要把追加到日志文件中的数据实时采集到hdfs</p></li><li><p>根据需求，首先定义以下3大要素</p><ul><li>采集源，即source——监控文件内容更新 :  exec  ‘tail -F file’</li><li>下沉目标，即sink——HDFS文件系统  :  hdfs sink</li><li>Source和sink之间的传递通道——channel，可用file channel 也可以用 内存channel</li></ul></li><li><p>配置文件编写：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">agent1.sources = source1</span><br><span class="line">agent1.sinks = sink1</span><br><span class="line">agent1.channels = channel1</span><br><span class="line"></span><br><span class="line"># Describe/configure tail -F source1</span><br><span class="line">agent1.sources.source1.type = exec</span><br><span class="line">agent1.sources.source1.command = tail -F /home/hadoop/logs/access_log</span><br><span class="line">agent1.sources.source1.channels = channel1</span><br><span class="line"></span><br><span class="line">#configure host for source</span><br><span class="line">agent1.sources.source1.interceptors = i1</span><br><span class="line">agent1.sources.source1.interceptors.i1.type = host</span><br><span class="line">agent1.sources.source1.interceptors.i1.hostHeader = hostname</span><br><span class="line"></span><br><span class="line"># Describe sink1</span><br><span class="line">agent1.sinks.sink1.type = hdfs</span><br><span class="line">#a1.sinks.k1.channel = c1</span><br><span class="line">agent1.sinks.sink1.hdfs.path =hdfs://hdp-node-01:9000/weblog/flume-collection/%y-%m-%d/%H-%M</span><br><span class="line">agent1.sinks.sink1.hdfs.filePrefix = access_log</span><br><span class="line">agent1.sinks.sink1.hdfs.maxOpenFiles = 5000</span><br><span class="line">agent1.sinks.sink1.hdfs.batchSize= 100</span><br><span class="line">agent1.sinks.sink1.hdfs.fileType = DataStream</span><br><span class="line">agent1.sinks.sink1.hdfs.writeFormat =Text</span><br><span class="line">agent1.sinks.sink1.hdfs.rollSize = 102400</span><br><span class="line">agent1.sinks.sink1.hdfs.rollCount = 1000000</span><br><span class="line">agent1.sinks.sink1.hdfs.rollInterval = 60</span><br><span class="line">agent1.sinks.sink1.hdfs.round = true</span><br><span class="line">agent1.sinks.sink1.hdfs.roundValue = 10</span><br><span class="line">agent1.sinks.sink1.hdfs.roundUnit = minute</span><br><span class="line">agent1.sinks.sink1.hdfs.useLocalTimeStamp = true</span><br><span class="line"></span><br><span class="line"># Use a channel which buffers events in memory</span><br><span class="line">agent1.channels.channel1.type = memory</span><br><span class="line">agent1.channels.channel1.keep-alive = 120</span><br><span class="line">agent1.channels.channel1.capacity = 500000</span><br><span class="line">agent1.channels.channel1.transactionCapacity = 600</span><br><span class="line"></span><br><span class="line"># Bind the source and sink to the channel</span><br><span class="line">agent1.sources.source1.channels = channel1</span><br><span class="line">agent1.sinks.sink1.channel = channel1</span><br></pre></td></tr></table></figure></li></ul><h2 id="更多source和sink组件"><a href="#更多source和sink组件" class="headerlink" title="更多source和sink组件"></a>更多source和sink组件</h2><p>Flume支持众多的source和sink类型，详细手册可参考官方文档<br><a href="http://flume.apache.org/FlumeUserGuide.html" target="_blank" rel="noopener">http://flume.apache.org/FlumeUserGuide.html</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Flume介绍&quot;&gt;&lt;a href=&quot;#Flume介绍&quot; class=&quot;headerlink&quot; title=&quot;Flume介绍&quot;&gt;&lt;/a&gt;Flume介绍&lt;/h2&gt;&lt;h3 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://yoursite.com/tags/Hadoop/"/>
    
      <category term="Java" scheme="http://yoursite.com/tags/Java/"/>
    
      <category term="Flume" scheme="http://yoursite.com/tags/Flume/"/>
    
  </entry>
  
  <entry>
    <title>Sqoop数据迁移</title>
    <link href="http://yoursite.com/2018/03/07/sqoop%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB/"/>
    <id>http://yoursite.com/2018/03/07/sqoop数据迁移/</id>
    <published>2018-03-07T03:16:28.000Z</published>
    <updated>2018-03-07T08:43:50.056Z</updated>
    
    <content type="html"><![CDATA[<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><ul><li>sqoop是apache旗下一款“Hadoop和关系数据库服务器之间传送数据”的工具。</li><li>导入数据：MySQL，Oracle导入数据到Hadoop的HDFS、HIVE、HBASE等数据存储系统；</li><li>导出数据：从Hadoop的文件系统中导出数据到关系数据库</li></ul><p><img src="http://ou9crezlk.bkt.clouddn.com/blog/180307/FjcLH2jC95.png?imageslim" alt="mark"></p><h2 id="工作机制"><a href="#工作机制" class="headerlink" title="工作机制"></a>工作机制</h2><ul><li>将导入或导出命令翻译成mapreduce程序来实现</li><li>在翻译出的mapreduce中主要是对inputformat和outputformat进行定制</li></ul><h2 id="sqoop实战及原理"><a href="#sqoop实战及原理" class="headerlink" title="sqoop实战及原理"></a>sqoop实战及原理</h2><h3 id="sqoop安装"><a href="#sqoop安装" class="headerlink" title="sqoop安装"></a>sqoop安装</h3><ul><li>安装sqoop的前提是已经具备java和hadoop的环境</li></ul><h4 id="1-下载并解压"><a href="#1-下载并解压" class="headerlink" title="1. 下载并解压"></a>1. 下载并解压</h4><ul><li>最新版下载地址<a href="http://ftp.wayne.edu/apache/sqoop/1.4.6/" target="_blank" rel="noopener">http://ftp.wayne.edu/apache/sqoop/1.4.6/</a></li></ul><h4 id="2-修改配置文件"><a href="#2-修改配置文件" class="headerlink" title="2. 修改配置文件"></a>2. 修改配置文件</h4><ul><li><p><code>$ cd $SQOOP_HOME/conf</code></p></li><li><p><code>$ mv sqoop-env-template.sh sqoop-env.sh</code></p></li><li><p>打开sqoop-env.sh并编辑下面几行：</p></li><li><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export HADOOP_COMMON_HOME=/home/hadoop/apps/hadoop-2.6.1/ </span><br><span class="line">export HADOOP_MAPRED_HOME=/home/hadoop/apps/hadoop-2.6.1/</span><br><span class="line">export HIVE_HOME=/home/hadoop/apps/hive-1.2.1</span><br></pre></td></tr></table></figure></li></ul><p>####3. 加入mysql的jdbc驱动包</p><ul><li><code>cp  ~/app/hive/lib/mysql-connector-java-5.1.28.jar   $SQOOP_HOME/lib/</code></li></ul><h4 id="4-验证启动"><a href="#4-验证启动" class="headerlink" title="4. 验证启动"></a>4. 验证启动</h4><ul><li><p><code>$ cd $SQOOP_HOME/bin</code><br><code>$ sqoop-version</code></p></li><li><p>预期的输出：</p><ul><li><figure class="highlight verilog"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">15</span>/<span class="number">12</span>/<span class="number">17</span> <span class="number">14</span>:<span class="number">52</span>:<span class="number">32</span> INFO sqoop<span class="variable">.Sqoop</span>: Running Sqoop version: <span class="number">1</span><span class="variable">.4</span><span class="variable">.6</span></span><br><span class="line">Sqoop <span class="number">1</span><span class="variable">.4</span><span class="variable">.6</span> git commit id <span class="number">5</span>b34accaca7de251fc91161733f906af2eddbe83</span><br><span class="line">Compiled by abe on Fri Aug <span class="number">1</span> <span class="number">11</span>:<span class="number">19</span>:<span class="number">26</span> PDT <span class="number">2015</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>到这里，整个Sqoop安装工作完成。</p></li></ul><h2 id="Sqoop的数据导入"><a href="#Sqoop的数据导入" class="headerlink" title="Sqoop的数据导入"></a>Sqoop的数据导入</h2><ul><li>“导入工具”导入单个表从RDBMS到HDFS。表中的每一行被视为HDFS的记录。所有记录都存储为文本文件的文本数据（或者Avro、sequence文件等二进制数据） </li></ul><h3 id="语法"><a href="#语法" class="headerlink" title="语法"></a>语法</h3><ul><li>下面的语法用于将数据导入HDFS。</li></ul><p><code>$ sqoop import (generic-args) (import-args)</code></p><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><h4 id="表数据"><a href="#表数据" class="headerlink" title="表数据"></a>表数据</h4><ul><li><p>在mysql中有一个库userdb中三个表：emp, emp_add和emp_contact</p></li><li><p>表emp:</p><p>| id   | name     | deg          | salary | dept |<br>| —- | ——– | ———— | —— | —- |<br>| 1201 | gopal    | manager      | 50,000 | TP   |<br>| 1202 | manisha  | Proof reader | 30,000 | TP   |<br>| 1203 | khalil   | php dev      | 30,000 | AC   |<br>| 1204 | prasanth | php dev      | 20,000 | AC   |<br>| 1205 | kranthi  | admin        | 20,000 | TP   |</p></li><li><p>表emp_add:</p><p>| id   | hno  | street   | city    |<br>| —- | —- | ——– | ——- |<br>| 1201 | 288A | vgiri    | jublee  |<br>| 1202 | 108I | aoc      | sec-bad |<br>| 1203 | 144Z | pgutta   | hyd     |<br>| 1204 | 78B  | old city | sec-bad |<br>| 1205 | 720X | hitec    | sec-bad |</p></li><li><p>表emp_conn:</p><p>| id   | phno    | email           |<br>| —- | ——- | ————— |<br>| 1201 | 2356742 | <a href="mailto:gopal@tp.com" target="_blank" rel="noopener">gopal@tp.com</a>    |<br>| 1202 | 1661663 | <a href="mailto:manisha@tp.com" target="_blank" rel="noopener">manisha@tp.com</a>  |<br>| 1203 | 8887776 | <a href="mailto:khalil@ac.com" target="_blank" rel="noopener">khalil@ac.com</a>   |<br>| 1204 | 9988774 | <a href="mailto:prasanth@ac.com" target="_blank" rel="noopener">prasanth@ac.com</a> |<br>| 1205 | 1231231 | <a href="mailto:kranthi@tp.com" target="_blank" rel="noopener">kranthi@tp.com</a>  |</p></li></ul><h3 id="导入表表数据到HDFS"><a href="#导入表表数据到HDFS" class="headerlink" title="导入表表数据到HDFS"></a>导入表表数据到HDFS</h3><ul><li><p>下面的命令用于从MySQL数据库服务器中的emp表导入HDFS。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> bin/sqoop import   \</span><br><span class="line">--connect jdbc:mysql://hdp-node-01:3306/test   \</span><br><span class="line">--username root  \</span><br><span class="line">--password root   \</span><br><span class="line">--table emp   \</span><br><span class="line">--m 1</span><br></pre></td></tr></table></figure></li><li><p>如果成功执行，那么会得到下面的输出。</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">14/12/22 15:24:54 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5</span><br><span class="line">14/12/22 15:24:56 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.</span><br><span class="line">INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/cebe706d23ebb1fd99c1f063ad51ebd7/emp.jar</span><br><span class="line">-----------------------------------------------------</span><br><span class="line">O mapreduce.Job: map 0% reduce 0%</span><br><span class="line">14/12/22 15:28:08 INFO mapreduce.Job: map 100% reduce 0%</span><br><span class="line">14/12/22 15:28:16 INFO mapreduce.Job: Job job_1419242001831_0001 completed successfully</span><br><span class="line">-----------------------------------------------------</span><br><span class="line">-----------------------------------------------------</span><br><span class="line">14/12/22 15:28:17 INFO mapreduce.ImportJobBase: Transferred 145 bytes in 177.5849 seconds (0.8165 bytes/sec)</span><br><span class="line">14/12/22 15:28:17 INFO mapreduce.ImportJobBase: Retrieved 5 records.</span><br></pre></td></tr></table></figure><ul><li><p>为了验证在HDFS导入的数据，请使用以下命令查看导入的数据</p></li><li><p><code>$ $HADOOP_HOME/bin/hadoop fs -cat /user/hadoop/emp/part-m-00000</code></p></li><li><p>emp表的数据和字段之间用逗号(,)表示。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1201, gopal,    manager, 50000, TP</span><br><span class="line">1202, manisha,  preader, 50000, TP</span><br><span class="line">1203, kalil,    php dev, 30000, AC</span><br><span class="line">1204, prasanth, php dev, 30000, AC</span><br><span class="line">1205, kranthi,  admin,   20000, TP</span><br></pre></td></tr></table></figure></li></ul><h3 id="导入关系表到HIVE"><a href="#导入关系表到HIVE" class="headerlink" title="导入关系表到HIVE"></a>导入关系表到HIVE</h3><ul><li><code>bin/sqoop import --connect jdbc:mysql://hdp-node-01:3306/test --username root --password root --table emp --hive-import --m 1</code></li></ul><h3 id="导入到HDFS指定目录"><a href="#导入到HDFS指定目录" class="headerlink" title="导入到HDFS指定目录"></a>导入到HDFS指定目录</h3><ul><li>在导入表数据到HDFS使用Sqoop导入工具，我们可以指定目标目录。</li><li>以下是指定目标目录选项的Sqoop导入命令的语法。<ul><li><code>--target-dir &lt;new or exist directory in HDFS&gt;</code></li></ul></li><li>下面的命令是用来导入emp_add表数据到’/queryresult’目录。</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://hdp-node-01:3306/test \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--target-dir /queryresult \</span><br><span class="line">--table emp --m 1</span><br></pre></td></tr></table></figure><ul><li>下面的命令是用来验证 /queryresult 目录中 emp_add表导入的数据形式。<ul><li><code>$HADOOP_HOME/bin/hadoop fs -cat /queryresult/part-m-*</code></li></ul></li><li>它会用逗号（，）分隔emp_add表的数据和字段。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1201, 288A, vgiri,   jublee</span><br><span class="line">1202, 108I, aoc,     sec-bad</span><br><span class="line">1203, 144Z, pgutta,  hyd</span><br><span class="line">1204, 78B,  oldcity, sec-bad</span><br><span class="line">1205, 720C, hitech,  sec-bad</span><br></pre></td></tr></table></figure><h3 id="导入表数据子集"><a href="#导入表数据子集" class="headerlink" title="导入表数据子集"></a>导入表数据子集</h3><ul><li><p>我们可以导入表的使用Sqoop导入工具，”where”子句的一个子集。它执行在各自的数据库服务器相应的SQL查询，并将结果存储在HDFS的目标目录。</p></li><li><p>where子句的语法如下。</p><ul><li><code>--where &lt;condition&gt;</code></li></ul></li><li><p>下面的命令用来导入emp_add表数据的子集。子集查询检索员工ID和地址，居住城市为：Secunderabad</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://hdp-node-01:3306/test \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--where "city ='sec-bad'" \</span><br><span class="line">--target-dir /wherequery \</span><br><span class="line">--table emp_add --m 1</span><br></pre></td></tr></table></figure></li><li><p>按需导入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://hdp-node-01:3306/test \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--target-dir /wherequery2 \</span><br><span class="line">--query 'select id,name,deg from emp WHERE  id&gt;1207 and $CONDITIONS' \</span><br><span class="line">--split-by id \</span><br><span class="line">--fields-terminated-by '\t' \</span><br><span class="line">--m 1</span><br></pre></td></tr></table></figure></li><li><p>下面的命令用来验证数据从emp_add表导入/wherequery目录</p><ul><li><code>$HADOOP_HOME/bin/hadoop fs -cat /wherequery/part-m-*</code></li></ul></li><li><p>它用逗号（，）分隔 emp_add表数据和字段。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1202, 108I, aoc, sec-bad</span><br><span class="line">1204, 78B, oldcity, sec-bad</span><br><span class="line">1205, 720C, hitech, sec-bad</span><br></pre></td></tr></table></figure></li></ul><h3 id="增量导入"><a href="#增量导入" class="headerlink" title="增量导入"></a>增量导入</h3><ul><li><p>增量导入是仅导入新添加的表中的行的技术。</p></li><li><p>它需要添加<code>incremental</code>, <code>check-column</code>, 和 <code>ast-value</code>选项来执行增量导入。</p></li><li><p>下面的语法用于Sqoop导入命令增量选项。</p><ul><li><code>--incremental &lt;mode&gt;</code></li><li><code>--check-column &lt;column name&gt;</code></li><li><code>--last value &lt;last check column value&gt;</code></li></ul></li><li><p>假设新添加的数据转换成emp表如下：</p><ul><li>1206, satish p, grp des, 20000, GR</li></ul></li><li><p>下面的命令用于在EMP表执行增量导入。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop import \</span><br><span class="line">--connect jdbc:mysql://hdp-node-01:3306/test \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table emp --m 1 \</span><br><span class="line">--incremental append \</span><br><span class="line">--check-column id \</span><br><span class="line">--last-value 1208</span><br></pre></td></tr></table></figure></li><li><p>以下命令用于从emp表导入HDFS emp/ 目录的数据验证。</p><ul><li><code>$ $HADOOP_HOME/bin/hadoop fs -cat /user/hadoop/emp/part-m-*</code></li></ul></li><li><p>它用逗号（，）分隔 emp_add表数据和字段。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1201, gopal,    manager, 50000, TP</span><br><span class="line">1202, manisha,  preader, 50000, TP</span><br><span class="line">1203, kalil,    php dev, 30000, AC</span><br><span class="line">1204, prasanth, php dev, 30000, AC</span><br><span class="line">1205, kranthi,  admin,   20000, TP</span><br><span class="line">1206, satish p, grp des, 20000, GR</span><br></pre></td></tr></table></figure></li><li><p>下面的命令是从表emp 用来查看修改或新添加的行</p><ul><li><code>$ $HADOOP_HOME/bin/hadoop fs -cat /emp/part-m-*1</code></li><li>这表示新添加的行用逗号（，）分隔emp表的字段。 </li><li><code>1206, satish p, grp des, 20000, GR</code></li></ul></li></ul><h2 id="Sqoop的数据导出"><a href="#Sqoop的数据导出" class="headerlink" title="Sqoop的数据导出"></a>Sqoop的数据导出</h2><ul><li>将数据从HDFS导出到RDBMS数据库</li><li>导出前，目标表必须存在于目标数据库中。<ul><li>默认操作是从将文件中的数据使用INSERT语句插入到表中</li><li>更新模式下，是生成UPDATE语句更新表数据</li></ul></li></ul><h3 id="语法-1"><a href="#语法-1" class="headerlink" title="语法"></a>语法</h3><ul><li>以下是export命令语法。<ul><li><code>$ sqoop export (generic-args) (export-args)</code></li></ul></li></ul><h3 id="示例-1"><a href="#示例-1" class="headerlink" title="示例"></a>示例</h3><ul><li>数据是在HDFS 中“EMP/”目录的emp_data文件中。所述emp_data如下：</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1201, gopal,     manager, 50000, TP</span><br><span class="line">1202, manisha,   preader, 50000, TP</span><br><span class="line">1203, kalil,     php dev, 30000, AC</span><br><span class="line">1204, prasanth,  php dev, 30000, AC</span><br><span class="line">1205, kranthi,   admin,   20000, TP</span><br><span class="line">1206, satish p,  grp des, 20000, GR</span><br></pre></td></tr></table></figure><ol><li><p>首先需要手动创建mysql中的目标表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ mysql</span><br><span class="line">mysql&gt; USE db;</span><br><span class="line">mysql&gt; CREATE TABLE employee ( </span><br><span class="line">   id INT NOT NULL PRIMARY KEY, </span><br><span class="line">   name VARCHAR(20), </span><br><span class="line">   deg VARCHAR(20),</span><br><span class="line">   salary INT,</span><br><span class="line">   dept VARCHAR(10));</span><br></pre></td></tr></table></figure></li><li><p>然后执行导出命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">bin/sqoop export \</span><br><span class="line">--connect jdbc:mysql://hdp-node-01:3306/test \</span><br><span class="line">--username root \</span><br><span class="line">--password root \</span><br><span class="line">--table employee \</span><br><span class="line">--export-dir /user/hadoop/emp/</span><br></pre></td></tr></table></figure></li><li><p>验证表mysql命令行。</p><ul><li><p><code>mysql&gt;select * from employee;</code></p></li><li><p>如果给定的数据存储成功，那么可以找到数据在如下的employee表。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">+------+--------------+-------------+-------------------+--------+</span><br><span class="line">| Id   | Name         | Designation | Salary            | Dept   |</span><br><span class="line">+------+--------------+-------------+-------------------+--------+</span><br><span class="line">| 1201 | gopal        | manager     | 50000             | TP     |</span><br><span class="line">| 1202 | manisha      | preader     | 50000             | TP     |</span><br><span class="line">| 1203 | kalil        | php dev     | 30000             | AC    </span><br><span class="line">| 1204 | prasanth     | php dev     | 30000             | AC     |</span><br><span class="line">| 1205 | kranthi      | admin       | 20000             | TP     |</span><br><span class="line">| 1206 | satish p     | grp des     | 20000             | GR     |</span><br><span class="line">+------+--------------+-------------+-------------------+--------+</span><br></pre></td></tr></table></figure></li></ul></li></ol><h2 id="Sqoop的原理"><a href="#Sqoop的原理" class="headerlink" title="Sqoop的原理"></a>Sqoop的原理</h2><h3 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h3><ul><li>Sqoop的原理其实就是将导入导出命令转化为mapreduce程序来执行，sqoop在接收到命令后，都要生成mapreduce程序</li><li>使用sqoop的代码生成工具可以方便查看到sqoop所生成的java代码，并可在此基础之上进行深入定制开发</li></ul><h3 id="代码定制"><a href="#代码定制" class="headerlink" title="代码定制"></a>代码定制</h3><ul><li><p>以下是Sqoop代码生成命令的语法：</p><ul><li><code>$ sqoop-codegen (generic-args) (codegen-args)</code></li><li><code>$ sqoop-codegen (generic-args) (codegen-args)</code></li></ul></li><li><p>示例：以USERDB数据库中的表emp来生成Java代码为例。</p></li><li><p>下面的命令用来生成导入</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span> sqoop-codegen \</span><br><span class="line">--import</span><br><span class="line">--connect jdbc:mysql://localhost/userdb \</span><br><span class="line">--username root \ </span><br><span class="line">--table emp</span><br></pre></td></tr></table></figure></li><li><p>如果命令成功执行，那么它就会产生如下的输出。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">14/12/23 02:34:40 INFO sqoop.Sqoop: Running Sqoop version: 1.4.5</span><br><span class="line">14/12/23 02:34:41 INFO tool.CodeGenTool: Beginning code generation</span><br><span class="line">……………….</span><br><span class="line">14/12/23 02:34:42 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /usr/local/hadoop</span><br><span class="line">Note: /tmp/sqoop-hadoop/compile/9a300a1f94899df4a9b10f9935ed9f91/emp.java uses or overrides a deprecated API.</span><br><span class="line">Note: Recompile with -Xlint:deprecation for details.</span><br><span class="line">14/12/23 02:34:47 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-hadoop/compile/9a300a1f94899df4a9b10f9935ed9f91/emp.jar</span><br></pre></td></tr></table></figure></li><li><p>验证: 查看输出目录下的文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>如果想做深入定制导出，则可修改上述代码文件</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;sqoop是apache旗下一款“Hadoop和关系数据库服务器之间传送数据”的工具。&lt;/li&gt;
&lt;li&gt;导入数据：MySQL，
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://yoursite.com/tags/Hadoop/"/>
    
      <category term="Java" scheme="http://yoursite.com/tags/Java/"/>
    
      <category term="Sqoop" scheme="http://yoursite.com/tags/Sqoop/"/>
    
  </entry>
  
  <entry>
    <title>Centos升级Python 2.7.12并安装最新pip</title>
    <link href="http://yoursite.com/2018/03/06/Centos%E5%8D%87%E7%BA%A7Python%202.7.12%E5%B9%B6%E5%AE%89%E8%A3%85%E6%9C%80%E6%96%B0pip/"/>
    <id>http://yoursite.com/2018/03/06/Centos升级Python 2.7.12并安装最新pip/</id>
    <published>2018-03-06T03:50:35.000Z</published>
    <updated>2018-03-06T07:32:42.870Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Centos升级Python-2-7-12并安装最新pip"><a href="#Centos升级Python-2-7-12并安装最新pip" class="headerlink" title="Centos升级Python 2.7.12并安装最新pip"></a>Centos升级Python 2.7.12并安装最新pip</h3><p>Centos系统一般默认就安装有Python2.6.6版本，不少软件需要2.7以上的，通过包管理工具安装不了最新的版本，通过源码编译可以方便安装指定版本，只需要把下面版本的数字换成你想要的版本号。</p><p>1.安装步骤<br>下载源码</p><p><code>wget http://www.python.org/ftp/python/2.7.12/Python-2.7.12.tgz</code> 在下载目录解压源码</p><p><code>tar -zxvf Python-2.7.12.tgz</code> 进入解压后的文件夹</p><p><code>cd Python-2.7.12</code> 在编译前先在/usr/local建一个文件夹python2.7.12（作为python的安装路径，以免覆盖老的版本，新旧版本可以共存的)</p><p><code>mkdir /usr/local/python2.7.12</code> ，编译前需要安装下面依赖，否则下面安装pip就会出错</p><p><code>yum install openssl openssl-devel zlib-devel gcc -y</code></p><p>安装完依赖后执行下面命令</p><p><code>vim ./Modules/Setup</code></p><p>找到#zlib zlibmodule.c -I$(prefix)/include -L$(exec_prefix)/lib -lz去掉注释并保存(即去掉井号)</p><p>在解压缩后的目录下编译安装</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">./configure --prefix=/usr/local/python2.7.12 --with-zlib</span><br><span class="line">make  </span><br><span class="line">make install</span><br></pre></td></tr></table></figure><p>此时没有覆盖老版本，再将原来/usr/bin/python链接改为别的名字</p><p><code>mv /usr/bin/python /usr/bin/python2.6.6</code> 再建立新版本python的软链接</p><p><code>ln -s /usr/local/python2.7.12/bin/python2.7 /usr/bin/python</code></p><p>这个时候输入</p><p><code>python</code></p><p>就会显示出python的新版本信息</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Python 2.7.12 (default, Oct 13 2016, 03:17:14)  </span><br><span class="line">[GCC 4.4.7 20120313 (Red Hat 4.4.7-17)] on linux2</span><br><span class="line">Type “help”, “copyright”, “credits” or “license” for more information.</span><br></pre></td></tr></table></figure><p>2.修改yum配置文件<br>之所以要保留旧版本，因为yum依赖Python2.6，改下yum的配置文件，指定旧的Python版本就可以了。</p><p><code>vim /usr/bin/yum</code>， 将第一行的#!/usr/bin/python修改成#!/usr/bin/python2.6.6</p><p>3.安装最新版本的pip<br><code>wget https://bootstrap.pypa.io/get-pip.py</code></p><p><code>python get-pip.py</code> 找到pip2.7的路径</p><p><code>find / -name &quot;pip*&quot;</code></p><p>上面的命令输出 /root/.cache/pip 这里省略一堆输出</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/python2.7.12/bin/pip</span><br><span class="line">/usr/local/python2.7.12/bin/pip2</span><br><span class="line">/usr/local/python2.7.12/bin/pip2.7 #就是这个</span><br><span class="line">/usr/bin/pip</span><br><span class="line">/usr/bin/pip2</span><br><span class="line">/usr/bin/pip2.6</span><br></pre></td></tr></table></figure><p>为其创建软链作为系统默认的启动版本（之前有旧版本的话就先删掉rm -rf /usr/bin/pip）</p><p><code>ln -s /usr/local/python2.7.12/bin/pip2.7 /usr/bin/pip</code> 看下pip的版本</p><p><code>pip -V</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip 8.1.2 from /usr/local/python2.7.12/lib/python2.7/site-packages (python 2.7)</span><br></pre></td></tr></table></figure><p>pip安装完毕，现在可以用它下载安装各种包了</p><p>我把上面的所有写成下面简单的脚本，一键就可以升级好。</p><p><code>wget http://7xpt4s.com1.z0.glb.clouddn.com/update-python2.7.12.sh &amp;&amp; bash update-python2.7.12.sh</code></p><p>参考：</p><p><a href="https://ruter.github.io/2015/12/03/Update-python/" target="_blank" rel="noopener">https://ruter.github.io/2015/12/03/Update-python/</a></p><p><a href="https://blog.phpgao.com/pip-easy_install-setuptool.html" target="_blank" rel="noopener">https://blog.phpgao.com/pip-easy_install-setuptool.html</a></p><p>原文地址：<a href="https://blog.fazero.me/2016/10/13/centos-update-python/" target="_blank" rel="noopener">https://blog.fazero.me/2016/10/13/centos-update-python/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Centos升级Python-2-7-12并安装最新pip&quot;&gt;&lt;a href=&quot;#Centos升级Python-2-7-12并安装最新pip&quot; class=&quot;headerlink&quot; title=&quot;Centos升级Python 2.7.12并安装最新pip&quot;&gt;&lt;/a
      
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="Python" scheme="http://yoursite.com/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>CentOS6.7安装JDK+Tomcat</title>
    <link href="http://yoursite.com/2018/03/06/CentOS6.7%E5%AE%89%E8%A3%85JDK+Tomcat/"/>
    <id>http://yoursite.com/2018/03/06/CentOS6.7安装JDK+Tomcat/</id>
    <published>2018-03-06T03:50:35.000Z</published>
    <updated>2018-03-06T07:32:12.320Z</updated>
    
    <content type="html"><![CDATA[<p>##安装JDK</p><ul><li>下载：jdk-7u45-linux-x64.tar.gz</li><li>上传到服务器</li><li>解压jdk到/usr/local目录：<ul><li><code>tar -zxvf jdk-7u45-linux-x64.tar.gz -C /usr/local</code></li></ul></li><li><p>设置环境变量，在/etc/profile文件最后追加相关内容</p><ul><li>vi /etc/profile<blockquote><p>export JAVA_HOME=/usr/local/jdk1.7.0_45<br>xport PATH=\$PATH:\$JAVA_HOME/bin</p></blockquote></li></ul></li><li><p>刷新环境变量</p><ul><li>source /etc/profile</li></ul></li><li>测试java命令是否可用<ul><li><code>java -version</code></li><li><img src="http://ou9crezlk.bkt.clouddn.com/blog/170906/9igaE4cFKj.png" alt="mark"></li><li>问题解决 ： <code>yum install -y glibc.i686</code></li></ul></li></ul><p>##安装JDK</p><ul><li>上传apache-tomcat-7.0.79.tar.gz到服务器上</li><li>解压缩到/usr/local/目录下：<ul><li><code>tar -zxvf apache-tomcat-7.0.79.tar.gz -C /usr/local/</code></li></ul></li><li>启动tomcat<ul><li><code>/usr/local/apache-tomcat-7.0.79/bin/startup.sh</code>  </li><li><img src="http://ou9crezlk.bkt.clouddn.com/blog/170906/4FGHGIfgKB.png" alt="mark"></li></ul></li><li>查看tomcat进程是否启动<ul><li><code>jps</code></li></ul></li><li>查看tomcat进程端口<ul><li><code>netstat -anpt | grep 8080</code></li><li><img src="http://ou9crezlk.bkt.clouddn.com/blog/170906/5em9a0AJeJ.png" alt="mark"></li></ul></li><li>通过浏览器访问tomcat<ul><li>http://服务器IP:8080/ </li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;##安装JDK&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;下载：jdk-7u45-linux-x64.tar.gz&lt;/li&gt;
&lt;li&gt;上传到服务器&lt;/li&gt;
&lt;li&gt;解压jdk到/usr/local目录：&lt;ul&gt;
&lt;li&gt;&lt;code&gt;tar -zxvf jdk-7u45-linux-x64
      
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="Centos 6" scheme="http://yoursite.com/tags/Centos-6/"/>
    
      <category term="JDK" scheme="http://yoursite.com/tags/JDK/"/>
    
      <category term="Tomcat" scheme="http://yoursite.com/tags/Tomcat/"/>
    
  </entry>
  
  <entry>
    <title>CentOS 7开放端口和关闭防火墙</title>
    <link href="http://yoursite.com/2018/03/06/CentOS%207%E5%BC%80%E6%94%BE%E7%AB%AF%E5%8F%A3%E5%92%8C%E5%85%B3%E9%97%AD%E9%98%B2%E7%81%AB%E5%A2%99/"/>
    <id>http://yoursite.com/2018/03/06/CentOS 7开放端口和关闭防火墙/</id>
    <published>2018-03-06T03:50:35.000Z</published>
    <updated>2018-03-06T07:31:55.500Z</updated>
    
    <content type="html"><![CDATA[<h1 id="开放端口"><a href="#开放端口" class="headerlink" title="开放端口"></a>开放端口</h1><p>永久的开放需要的端口</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo firewall-cmd --zone=public --add-port=3306/tcp --permanent</span><br><span class="line">sudo firewall-cmd --reload</span><br></pre></td></tr></table></figure><p>之后检查新的防火墙规则</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --list-all</span><br></pre></td></tr></table></figure><h1 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h1><p>由于只是用于开发环境，所以打算把防火墙关闭掉</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">//临时关闭防火墙,重启后会重新自动打开</span><br><span class="line">systemctl restart firewalld</span><br><span class="line">//检查防火墙状态</span><br><span class="line">firewall-cmd --state</span><br><span class="line">firewall-cmd --list-all</span><br><span class="line">//Disable firewall</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl status firewalld</span><br><span class="line">//Enable firewall</span><br><span class="line">systemctl enable firewalld</span><br><span class="line">systemctl start firewalld</span><br><span class="line">systemctl status firewalld</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;开放端口&quot;&gt;&lt;a href=&quot;#开放端口&quot; class=&quot;headerlink&quot; title=&quot;开放端口&quot;&gt;&lt;/a&gt;开放端口&lt;/h1&gt;&lt;p&gt;永久的开放需要的端口&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td 
      
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="Centos 7" scheme="http://yoursite.com/tags/Centos-7/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop学习遇到问题</title>
    <link href="http://yoursite.com/2018/03/06/Hadoop%E5%AD%A6%E4%B9%A0%E9%81%87%E5%88%B0%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2018/03/06/Hadoop学习遇到问题/</id>
    <published>2018-03-06T03:50:35.000Z</published>
    <updated>2018-03-06T07:45:27.323Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Hadoop集群警告"><a href="#Hadoop集群警告" class="headerlink" title="Hadoop集群警告"></a>Hadoop集群警告</h1><p>##查看集群状态</p><ul><li>hdfs dfsadmin -report</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">Configured Capacity: 55306051584 (51.51 GB)</span><br><span class="line">Present Capacity: 46777233408 (43.56 GB)</span><br><span class="line">DFS Remaining: 46777159680 (43.56 GB)</span><br><span class="line">DFS Used: 73728 (72 KB)</span><br><span class="line">DFS Used%: 0.00%</span><br><span class="line">Under replicated blocks: 0</span><br><span class="line">Blocks with corrupt replicas: 0</span><br><span class="line">Missing blocks: 0</span><br><span class="line"></span><br><span class="line">-------------------------------------------------</span><br><span class="line">Live datanodes (3):</span><br><span class="line"></span><br><span class="line">Name: 192.168.127.63:50010 (mini3)</span><br><span class="line">Hostname: mini3</span><br><span class="line">Decommission Status : Normal</span><br><span class="line">Configured Capacity: 18435350528 (17.17 GB)</span><br><span class="line">DFS Used: 24576 (24 KB)</span><br><span class="line">Non DFS Used: 2847330304 (2.65 GB)</span><br><span class="line">DFS Remaining: 15587995648 (14.52 GB)</span><br><span class="line">DFS Used%: 0.00%</span><br><span class="line">DFS Remaining%: 84.55%</span><br><span class="line">Configured Cache Capacity: 0 (0 B)</span><br><span class="line">Cache Used: 0 (0 B)</span><br><span class="line">Cache Remaining: 0 (0 B)</span><br><span class="line">Cache Used%: 100.00%</span><br><span class="line">Cache Remaining%: 0.00%</span><br><span class="line">Xceivers: 1</span><br><span class="line">Last contact: Tue Oct 24 00:10:37 CST 2017</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Name: 192.168.127.62:50010 (mini2)</span><br><span class="line">Hostname: mini2</span><br><span class="line">Decommission Status : Normal</span><br><span class="line">Configured Capacity: 18435350528 (17.17 GB)</span><br><span class="line">DFS Used: 24576 (24 KB)</span><br><span class="line">Non DFS Used: 2847182848 (2.65 GB)</span><br><span class="line">DFS Remaining: 15588143104 (14.52 GB)</span><br><span class="line">DFS Used%: 0.00%</span><br><span class="line">DFS Remaining%: 84.56%</span><br><span class="line">Configured Cache Capacity: 0 (0 B)</span><br><span class="line">Cache Used: 0 (0 B)</span><br><span class="line">Cache Remaining: 0 (0 B)</span><br><span class="line">Cache Used%: 100.00%</span><br><span class="line">Cache Remaining%: 0.00%</span><br><span class="line">Xceivers: 1</span><br><span class="line">Last contact: Tue Oct 24 00:10:37 CST 2017</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Name: 192.168.127.64:50010 (mini4)</span><br><span class="line">Hostname: mini4</span><br><span class="line">Decommission Status : Normal</span><br><span class="line">Configured Capacity: 18435350528 (17.17 GB)</span><br><span class="line">DFS Used: 24576 (24 KB)</span><br><span class="line">Non DFS Used: 2834305024 (2.64 GB)</span><br><span class="line">DFS Remaining: 15601020928 (14.53 GB)</span><br><span class="line">DFS Used%: 0.00%</span><br><span class="line">DFS Remaining%: 84.63%</span><br><span class="line">Configured Cache Capacity: 0 (0 B)</span><br><span class="line">Cache Used: 0 (0 B)</span><br><span class="line">Cache Remaining: 0 (0 B)</span><br><span class="line">Cache Used%: 100.00%</span><br><span class="line">Cache Remaining%: 0.00%</span><br><span class="line">Xceivers: 1</span><br><span class="line">Last contact: Tue Oct 24 00:10:37 CST 2017</span><br></pre></td></tr></table></figure><h2 id="警告的解决"><a href="#警告的解决" class="headerlink" title="警告的解决"></a>警告的解决</h2><ul><li><p>警告1</p><ul><li>Java HotSpot(TM) Client VM warning: You have loaded library /home/hujian/apps/hadoop-2.6.4/lib/native/libhadoop.so.1.0.0 which might have disabled stack guard. The VM will try to fix the stack guard now.</li><li>It’s highly recommended that you fix the library with ‘execstack -c <libfile>‘, or link it with ‘-z noexecstack’.</libfile></li></ul></li><li><p>解决方案：</p></li><li><p>在/etc/profile下添加如下两条环境变量</p><ul><li>export HADOOP_COMMON_LIB_NATIVE_DIR=${HADOOP_HOME}/lib/native</li><li>export HADOOP_OPTS=”-Djava.library.path=$HADOOP_HOME/lib”</li></ul></li><li><p>然后：</p><ul><li>source /etc/profile</li></ul></li><li><p>再启动hadoop</p></li><li><p>警告2：</p><ul><li>WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform… using builtin-java classes where applicable</li></ul></li><li><p>解决方案：</p><ul><li><p>直接在log4j日志中去除告警信息。在hadoop的安装目录 /apps/hadoop-2.5.2/etc/hadoop/log4j.properties文件中添加</p></li><li><blockquote><p>log4j.logger.org.apache.hadoop.util.NativeCodeLoader=ERROR</p></blockquote></li></ul></li></ul><h2 id="操作HDFS报错"><a href="#操作HDFS报错" class="headerlink" title="操作HDFS报错"></a>操作HDFS报错</h2><p>15/10/08 21:04:19 INFO hdfs.DFSClient: Exception in createBlockOutputStream<br>java.io.IOException: Got error, status message , ack with firstBadLink as 192.168.1.114:50010<br> at org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtoUtil.checkBlockOpStatus(DataTransferProtoUtil.java:140)<br> at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1334)<br> at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1237)</p><p>原因是由于centos7的防火墙引起与datanode无法通讯造成。</p><p>centos7 关闭防火墙 service firewalld stop</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Hadoop集群警告&quot;&gt;&lt;a href=&quot;#Hadoop集群警告&quot; class=&quot;headerlink&quot; title=&quot;Hadoop集群警告&quot;&gt;&lt;/a&gt;Hadoop集群警告&lt;/h1&gt;&lt;p&gt;##查看集群状态&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;hdfs dfsadmin -r
      
    
    </summary>
    
      <category term="Java" scheme="http://yoursite.com/categories/Java/"/>
    
    
      <category term="Hadoop" scheme="http://yoursite.com/tags/Hadoop/"/>
    
      <category term="Java" scheme="http://yoursite.com/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>HADOOP集群搭建</title>
    <link href="http://yoursite.com/2018/03/06/HADOOP%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA/"/>
    <id>http://yoursite.com/2018/03/06/HADOOP集群搭建/</id>
    <published>2018-03-06T03:50:35.000Z</published>
    <updated>2018-03-06T07:33:05.260Z</updated>
    
    <content type="html"><![CDATA[<ul><li><h1 id="HADOOP集群搭建"><a href="#HADOOP集群搭建" class="headerlink" title="HADOOP集群搭建"></a>HADOOP集群搭建</h1><h1 id="集群简介"><a href="#集群简介" class="headerlink" title="集群简介"></a>集群简介</h1><blockquote><ul><li>HADOOP集群具体来说包含两个集群：HDFS集群和YARN集群，两者逻辑上分离，但物理上常在一起</li></ul><ul><li>HDFS集群：<ul><li>负责海量数据的存储，集群中的角色主要有 NameNode / DataNode</li></ul></li><li>YARN集群：<ul><li>负责海量数据运算时的资源调度，集群中的角色主要有 ResourceManager /NodeManager</li></ul></li></ul><p><strong>(那mapreduce是什么呢？它其实是一个应用程序开发包)</strong></p></blockquote><p>​</p><h2 id="服务器准备"><a href="#服务器准备" class="headerlink" title="服务器准备"></a>服务器准备</h2><p>本案例使用虚拟机服务器来搭建HADOOP集群，</p><ul><li>服务器节点数量：3</li></ul></li></ul><ul><li><p>所用软件及版本：</p><ul><li>Vmware 11.0</li><li>Centos  6.5  64bit</li></ul><h2 id="网络环境准备"><a href="#网络环境准备" class="headerlink" title="网络环境准备"></a>网络环境准备</h2></li><li><p>采用NAT方式联网</p></li><li>网关地址：192.168.127.1</li><li>3个服务器节点IP地址：192.168.127.61、192.168.127.62、192.168.127.63</li><li><p>子网掩码：255.255.255.0</p><h2 id="服务器系统设置"><a href="#服务器系统设置" class="headerlink" title="服务器系统设置"></a>服务器系统设置</h2></li></ul><ol><li><p>添加HADOOP用户  useradd hadoop</p><ol><li><p>为HADOOP用户分配sudoer权限vi /etc/sudoers </p></li><li><blockquote><p>hadoop    ALL=(ALL)       ALL</p></blockquote></li></ol></li><li><p>同步时间</p></li><li><p>设置主机名</p><ol><li><blockquote><p>mini1</p><p>mini2</p><p>mini3</p></blockquote></li></ol></li><li><p>配置内网域名映射</p><ol><li><p><code>vi /etc/hosts</code></p></li><li><blockquote><p>192.168.127.61          mini1</p><p>192.168.127.62          mini2</p><p>192.168.127.63          mini3</p></blockquote></li></ol></li><li><p>配置ssh免密登陆</p></li><li><p>配置防火墙</p><h2 id="JDK环境安装"><a href="#JDK环境安装" class="headerlink" title="JDK环境安装"></a>JDK环境安装</h2></li></ol><ul><li>上传jdk安装包</li><li>解压安装包</li><li><p>配置环境变量 /etc/profile</p><h2 id="HADOOP安装部署"><a href="#HADOOP安装部署" class="headerlink" title="HADOOP安装部署"></a>HADOOP安装部署</h2></li><li><p>上传HADOOP安装包<strong>(Hadoop需要编译。Hadoop是使用Java语言开发的，但是有一些需求和操作并不适合使用java，所以就引入了本地库（Native Libraries）的概念)</strong></p></li><li><p>从<a href="https://link.jianshu.com/?t=http://hadoop.apache.org/releases.html" target="_blank" rel="noopener">http://hadoop.apache.org/releases.html</a>找到一个你想要的版本，下载binary包</p></li><li><p>规划安装目录  /home/hadoop/apps/hadoop-2.9.0</p></li><li><p>解压安装包</p><ul><li><code>tar -zxvf hadoop-2.9.0.tar.gz</code></li></ul></li><li><p>修改  <code>$HADOOP_HOME/etc/hadoop/</code>下的配置文件</p><ul><li><p>最简化配置如下:</p></li><li><p><code>vi  hadoop-env.sh</code></p><ul><li><blockquote><p># The java implementation to use.</p><p>export JAVA_HOME=/home/hadoop/apps/jdk1.7.0_65</p></blockquote></li></ul></li><li><p><code>vi  core-site.xml</code></p><ul><li><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 指定HADOOP所使用的文件系统schema（URI），HDFS的老大（NameNode）的地址 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://mini1:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="comment">&lt;!-- 指定hadoop运行时产生文件的存储目录 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/hdpdata<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p><code>vi  hdfs-site.xml</code></p><ul><li><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 指定HDFS副本的数量 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p><code>vi  mapred-site.xml</code></p><ul><li><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定mr运行在yarn上 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p><code>vi  yarn-site.xml</code></p><ul><li><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- 指定YARN的老大（ResourceManager）的地址 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mini1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="comment">&lt;!-- reducer获取数据的方式 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p><code>vi slaves</code></p><ul><li><blockquote><p>mini2</p><p>mini3</p></blockquote></li></ul></li></ul><h2 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h2></li><li><p>将配置好的hadoop文件夹复制到其他节点机器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r ~/apps hadoop@mini2:~/</span><br><span class="line">scp -r ~/apps hadoop@mini3:~/</span><br></pre></td></tr></table></figure></li><li><p>初始化HDFS(mini1机器)</p><ul><li><code>/home/hadoop/apps/hadoop-2.9.0/bin/hadoop namenode -format</code></li></ul></li><li><p>启动HDFS(mini1机器)</p><ul><li><p><code>/home/hadoop/apps/hadoop-2.9.0/sbin/start-dfs.sh</code></p></li><li><p><code>jps</code>命令检查是否启动成功</p><ul><li><blockquote><p>2136 SecondaryNameNode</p><p>2257 Jps</p><p>1792 NameNode</p></blockquote></li></ul></li></ul></li><li><p>启动YARN(mini1机器)</p><ul><li><p><code>/home/hadoop/apps/hadoop-2.6.4/sbin/start-yarn.sh</code></p></li><li><p><code>jps</code>命令检查是否启动成功</p><ul><li><blockquote><p>2306 ResourceManager</p><p>2621 Jps</p><p>2408 NodeManager</p><p>2136 SecondaryNameNode</p><p>1792 NameNode</p></blockquote></li></ul></li></ul></li><li><p>访问控制台<a href="http://mini1:50070" target="_blank" rel="noopener">http://mini1:50070</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ul&gt;
&lt;li&gt;&lt;h1 id=&quot;HADOOP集群搭建&quot;&gt;&lt;a href=&quot;#HADOOP集群搭建&quot; class=&quot;headerlink&quot; title=&quot;HADOOP集群搭建&quot;&gt;&lt;/a&gt;HADOOP集群搭建&lt;/h1&gt;&lt;h1 id=&quot;集群简介&quot;&gt;&lt;a href=&quot;#集群简介&quot; cl
      
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="Hadoop" scheme="http://yoursite.com/tags/Hadoop/"/>
    
  </entry>
  
  <entry>
    <title>Java-Nio学习</title>
    <link href="http://yoursite.com/2018/03/06/Java-nio/"/>
    <id>http://yoursite.com/2018/03/06/Java-nio/</id>
    <published>2018-03-06T03:50:35.000Z</published>
    <updated>2018-03-06T06:54:09.767Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Java-nio"><a href="#Java-nio" class="headerlink" title="Java-nio"></a>Java-nio</h1><h2 id="nio原理学习"><a href="#nio原理学习" class="headerlink" title="nio原理学习"></a>nio原理学习</h2><h3 id="nio简介"><a href="#nio简介" class="headerlink" title="nio简介"></a>nio简介</h3><blockquote><p>nio 是New IO 的简称，在jdk1.4 里提供的新api 。Sun 官方标榜的特性如下： 为所有的原始类型提供(Buffer)缓存支持。字符集编码解码解决方案。 Channel ：一个新的原始I/O 抽象。 支持锁和内存映射文件的文件访问接口。 提供多路(non-bloking) 非阻塞式的高伸缩性网络I/O 。</p></blockquote><h3 id="传统的I-O"><a href="#传统的I-O" class="headerlink" title="传统的I/O"></a>传统的I/O</h3><ul><li><p>使用传统的I/O程序读取文件内容, 并写入到另一个文件(或Socket), 如下程序:</p><ul><li><code>File.read(fileDesc, buf, len);</code></li><li><code>Socket.send(socket, buf, len);</code></li></ul></li><li><p>会有较大的性能开销, 主要表现在一下两方面:</p><ol><li><p>上下文切换(context switch), 此处有4次用户态和内核态的切换</p></li><li><p>Buffer内存开销, 一个是应用程序buffer, 另一个是系统读取buffer以及socket buffer其运行示意图如下</p><p>​</p></li></ol></li></ul><p><img src="http://ou9crezlk.bkt.clouddn.com/blog/170930/JcGb1GE8ce.png?imageslim" alt="mark"></p><ol><li>先将文件内容从磁盘中拷贝到操作系统buffer</li></ol><ol><li>再从操作系统buffer拷贝到程序应用buffer</li></ol><ol><li>从程序buffer拷贝到socket buffer</li><li>从socket buffer拷贝到协议引擎.</li></ol><h3 id="NIO"><a href="#NIO" class="headerlink" title="NIO"></a>NIO</h3><ul><li><p>NIO技术省去了将操作系统的read buffer拷贝到程序的buffer, 以及从程序buffer拷贝到socket buffer的步骤, 直接将 read buffer 拷贝到 socket buffer. java 的 <code>FileChannel.transferTo()</code> 方法就是这样的实现, 这个实现是依赖于操作系统底层的<code>sendFile()</code>实现的.</p></li><li><p><code>publicvoid transferTo(long position, long count, WritableByteChannel target);</code></p><p>他的底层调用的是系统调用sendFile()方法</p><p><code>sendfile(int out_fd, int in_fd, off_t *offset, size_t count);</code></p><p>如下图</p><p><img src="http://ou9crezlk.bkt.clouddn.com/blog/170930/3cdejCHKjI.png?imageslim" alt="mark"></p></li></ul><h3 id="传统socket和socket-nio代码"><a href="#传统socket和socket-nio代码" class="headerlink" title="传统socket和socket nio代码"></a>传统socket和socket nio代码</h3><ul><li><p>传统socket</p><ul><li><p>server</p></li><li><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String args[])</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">// 监听端口</span></span><br><span class="line">ServerSocket server_socket = <span class="keyword">new</span> ServerSocket(<span class="number">2000</span>);</span><br><span class="line">System.out.println(<span class="string">"等待，端口为："</span> + server_socket.getLocalPort());</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line"><span class="comment">// 阻塞接受消息</span></span><br><span class="line">Socket socket = server_socket.accept();</span><br><span class="line"><span class="comment">// 打印链接信息</span></span><br><span class="line">System.out.println(<span class="string">"新连接： "</span> + socket.getInetAddress() + <span class="string">":"</span></span><br><span class="line">+ socket.getPort());</span><br><span class="line"><span class="comment">// 从socket中获取流</span></span><br><span class="line">DataInputStream input = <span class="keyword">new</span> DataInputStream(socket.getInputStream());</span><br><span class="line"><span class="comment">// 接收数据</span></span><br><span class="line"><span class="keyword">byte</span>[] byteArray = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">4096</span>];</span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line"><span class="keyword">int</span> nread = input.read(byteArray, <span class="number">0</span>, <span class="number">4096</span>);</span><br><span class="line">System.out.println(<span class="keyword">new</span> String(byteArray, <span class="string">"UTF-8"</span>));</span><br><span class="line"><span class="keyword">if</span> (-<span class="number">1</span> == nread) &#123;</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">socket.close();</span><br><span class="line">System.out.println(<span class="string">"Connection closed by client"</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li><p>client</p></li><li><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">long</span> start = System.currentTimeMillis();</span><br><span class="line"><span class="comment">// 创建socket链接</span></span><br><span class="line">Socket socket = <span class="keyword">new</span> Socket(<span class="string">"localhost"</span>, <span class="number">2000</span>);</span><br><span class="line">System.out.println(<span class="string">"Connected with server "</span> + socket.getInetAddress()</span><br><span class="line">+ <span class="string">":"</span> + socket.getPort());</span><br><span class="line"><span class="comment">// 读取文件</span></span><br><span class="line">FileInputStream inputStream = <span class="keyword">new</span> FileInputStream(<span class="string">"C:/sss.txt"</span>);</span><br><span class="line"><span class="comment">// 输出文件</span></span><br><span class="line">DataOutputStream output = <span class="keyword">new</span> DataOutputStream(socket.getOutputStream());</span><br><span class="line"><span class="comment">// 缓冲区4096K</span></span><br><span class="line"><span class="keyword">byte</span>[] b = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">4096</span>];</span><br><span class="line"><span class="comment">// 传输长度</span></span><br><span class="line"><span class="keyword">long</span> read = <span class="number">0</span>, total = <span class="number">0</span>;</span><br><span class="line"><span class="comment">// 读取文件，写到socketio中</span></span><br><span class="line"><span class="keyword">while</span> ((read = inputStream.read(b)) &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">total = total + read;</span><br><span class="line">output.write(b);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 关闭</span></span><br><span class="line">output.close();</span><br><span class="line">socket.close();</span><br><span class="line">inputStream.close();</span><br><span class="line"><span class="comment">// 打印时间</span></span><br><span class="line">System.out.println(<span class="string">"bytes send--"</span> + total + <span class="string">" and totaltime--"</span></span><br><span class="line">+ (System.currentTimeMillis() - start));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>socket nio 代码</p><ul><li><p>server</p><ul><li><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="comment">// 创建socket channel</span></span><br><span class="line">ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();</span><br><span class="line">ServerSocket ss = serverSocketChannel.socket();</span><br><span class="line">ss.setReuseAddress(<span class="keyword">true</span>);<span class="comment">// 地址重用</span></span><br><span class="line">ss.bind(<span class="keyword">new</span> InetSocketAddress(<span class="string">"localhost"</span>, <span class="number">9026</span>));<span class="comment">// 绑定地址</span></span><br><span class="line">System.out.println(<span class="string">"监听端口 : "</span></span><br><span class="line">+ <span class="keyword">new</span> InetSocketAddress(<span class="string">"localhost"</span>, <span class="number">9026</span>).toString());</span><br><span class="line"></span><br><span class="line"><span class="comment">// 分配一个新的字节缓冲区</span></span><br><span class="line">ByteBuffer dst = ByteBuffer.allocate(<span class="number">4096</span>);</span><br><span class="line"><span class="comment">// 读取数据</span></span><br><span class="line"><span class="keyword">while</span> (<span class="keyword">true</span>) &#123;</span><br><span class="line">SocketChannel channle = serverSocketChannel.accept();<span class="comment">// 接收数据</span></span><br><span class="line">System.out.println(<span class="string">"Accepted : "</span> + channle);</span><br><span class="line">channle.configureBlocking(<span class="keyword">true</span>);<span class="comment">// 设置阻塞，接不到就停</span></span><br><span class="line"><span class="keyword">int</span> nread = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (nread != -<span class="number">1</span>) &#123;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">nread = channle.read(dst);<span class="comment">// 往缓冲区里读</span></span><br><span class="line"><span class="keyword">byte</span>[] array = dst.array();<span class="comment">//将数据转换为array</span></span><br><span class="line"><span class="comment">//打印</span></span><br><span class="line">String string = <span class="keyword">new</span> String(array, <span class="number">0</span>, dst.position());</span><br><span class="line">System.out.print(string);</span><br><span class="line">dst.clear();</span><br><span class="line">&#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">e.printStackTrace();</span><br><span class="line">nread = -<span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>client</p><ul><li><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line"><span class="keyword">long</span> start = System.currentTimeMillis();</span><br><span class="line"><span class="comment">// 打开socket的nio管道</span></span><br><span class="line">SocketChannel sc = SocketChannel.open();</span><br><span class="line">sc.connect(<span class="keyword">new</span> InetSocketAddress(<span class="string">"localhost"</span>, <span class="number">9026</span>));<span class="comment">// 绑定相应的ip和端口</span></span><br><span class="line">sc.configureBlocking(<span class="keyword">true</span>);<span class="comment">// 设置阻塞</span></span><br><span class="line"><span class="comment">// 将文件放到channel中</span></span><br><span class="line">FileChannel fc = <span class="keyword">new</span> FileInputStream(<span class="string">"C:/sss.txt"</span>).getChannel();<span class="comment">// 打开文件管道</span></span><br><span class="line"><span class="comment">//做好标记量</span></span><br><span class="line"><span class="keyword">long</span> size = fc.size();</span><br><span class="line"><span class="keyword">int</span> pos = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span> offset = <span class="number">4096</span>;</span><br><span class="line"><span class="keyword">long</span> curnset = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">long</span> counts = <span class="number">0</span>;</span><br><span class="line"><span class="comment">//循环写</span></span><br><span class="line"><span class="keyword">while</span> (pos&lt;size) &#123;</span><br><span class="line">curnset = fc.transferTo(pos, <span class="number">4096</span>, sc);<span class="comment">// 把文件直接读取到socket chanel中，返回文件大小</span></span><br><span class="line">pos+=offset;</span><br><span class="line">counts+=curnset;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//关闭</span></span><br><span class="line">fc.close();</span><br><span class="line">sc.close();</span><br><span class="line"><span class="comment">//打印传输字节数</span></span><br><span class="line">System.out.println(counts);</span><br><span class="line"><span class="comment">// 打印时间</span></span><br><span class="line">System.out.println(<span class="string">"bytes send--"</span> + counts + <span class="string">" and totaltime--"</span></span><br><span class="line">+ (System.currentTimeMillis() - start));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>​</p></li></ul></li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Java-nio&quot;&gt;&lt;a href=&quot;#Java-nio&quot; class=&quot;headerlink&quot; title=&quot;Java-nio&quot;&gt;&lt;/a&gt;Java-nio&lt;/h1&gt;&lt;h2 id=&quot;nio原理学习&quot;&gt;&lt;a href=&quot;#nio原理学习&quot; class=&quot;header
      
    
    </summary>
    
      <category term="Java" scheme="http://yoursite.com/categories/Java/"/>
    
    
      <category term="Java" scheme="http://yoursite.com/tags/Java/"/>
    
      <category term="Nio" scheme="http://yoursite.com/tags/Nio/"/>
    
  </entry>
  
  <entry>
    <title>Hive安装配置</title>
    <link href="http://yoursite.com/2018/03/06/Hive%E5%AE%89%E8%A3%85/"/>
    <id>http://yoursite.com/2018/03/06/Hive安装/</id>
    <published>2018-03-06T03:50:35.000Z</published>
    <updated>2018-03-07T09:09:24.833Z</updated>
    
    <content type="html"><![CDATA[<p>Hive是基于Hadoop构建的一套数据仓库分析系统，它提供了丰富的SQL查询方式来分析存储在Hadoop 分布式文件系统中的数据。其在Hadoop的架构体系中承担了一个SQL解析的过程，它提供了对外的入口来获取用户的指令然后对指令进行分析，解析出一个MapReduce程序组成可执行计划，并按照该计划生成对应的MapReduce任务提交给Hadoop集群处理，获取最终的结果。元数据——如表模式——存储在名为metastore的数据库中。</p><h3 id="系统环境"><a href="#系统环境" class="headerlink" title="系统环境"></a>系统环境</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">192.168.186.128 hadoop-master</span><br><span class="line">192.168.186.129 hadoop-slave</span><br><span class="line">MySQL安装在master机器上，hive服务器也安装在master上</span><br></pre></td></tr></table></figure><h3 id="Hive下载"><a href="#Hive下载" class="headerlink" title="Hive下载"></a>Hive下载</h3><p>下载源码包，最新版本可自行去官网下载</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-master ~]$ wget http://mirrors.cnnic.cn/apache/hive/hive-1.2.1/apache-hive-1.2.1-bin.tar.gz</span><br><span class="line">[hadoop@hadoop-master ~]$ tar -zxf apache-hive-1.2.1-bin.tar.gz </span><br><span class="line">[hadoop@hadoop-master ~]$ ls</span><br><span class="line">apache-hive-1.2.1-bin  apache-hive-1.2.1-bin.tar.gz  dfs  hadoop-2.7.1  Hsource  tmp</span><br></pre></td></tr></table></figure><p>配置环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-master hadoop]# vi /etc/profile</span><br><span class="line">HIVE_HOME=/home/hadoop/apache-hive-1.2.1-bin</span><br><span class="line">PATH=$PATH:$HIVE_HOME/bin</span><br><span class="line">export HIVE_NAME PATH</span><br><span class="line">[root@hadoop-master hadoop]# source /etc/profile</span><br></pre></td></tr></table></figure><h3 id="Metastore"><a href="#Metastore" class="headerlink" title="Metastore"></a>Metastore</h3><p>metastore是Hive元数据集中存放地。它包括两部分：服务和后台数据存储。有三种方式配置metastore：内嵌metastore、本地metastore以及远程metastore。<br>本次搭建中采用MySQL作为远程仓库，部署在hadoop-master节点上，hive服务端也安装在hive-master上，hive客户端即hadoop-slave访问hive服务器。</p><h4 id="MySQL安装"><a href="#MySQL安装" class="headerlink" title="MySQL安装"></a>MySQL安装</h4><h5 id="安装依赖包"><a href="#安装依赖包" class="headerlink" title="安装依赖包"></a>安装依赖包</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"># yum install gcc gcc-c++ ncurses-devel  -y</span><br></pre></td></tr></table></figure><h5 id="安装cmake"><a href="#安装cmake" class="headerlink" title="安装cmake"></a>安装cmake</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># wget http://www.cmake.org/files/v2.8/cmake-2.8.12.tar.gz</span><br><span class="line"># tar zxvf cmake-2.8.12.tar.gz</span><br><span class="line"># cd cmake-2.8.12</span><br><span class="line"># ./bootstrap </span><br><span class="line"># make &amp;&amp; make install</span><br></pre></td></tr></table></figure><h5 id="创建用户的相应目录"><a href="#创建用户的相应目录" class="headerlink" title="创建用户的相应目录"></a>创建用户的相应目录</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># groupadd mysql</span><br><span class="line"># useradd -g mysql mysql</span><br><span class="line"># mkdir -p /data/mysql/</span><br><span class="line"># mkdir -p /data/mysql/data/</span><br><span class="line"># mkdir -p /data/mysql/log/</span><br></pre></td></tr></table></figure><h5 id="获取MySQL安装包并安装"><a href="#获取MySQL安装包并安装" class="headerlink" title="获取MySQL安装包并安装"></a>获取MySQL安装包并安装</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># wget http://dev.mysql.com/get/downloads/mysql/mysql-5.6.25.tar.gz</span><br><span class="line"># tar zxvf mysql-5.6.25.tar.gz</span><br><span class="line"># cd mysql-5.6.25</span><br><span class="line"># cmake \</span><br><span class="line">-DCMAKE_INSTALL_PREFIX=/data/mysql \</span><br><span class="line">-DMYSQL_UNIX_ADDR=/data/mysql/mysql.sock \</span><br><span class="line">-DDEFAULT_CHARSET=utf8 \</span><br><span class="line">-DDEFAULT_COLLATION=utf8_general_ci \</span><br><span class="line">-DWITH_INNOBASE_STORAGE_ENGINE=1 \</span><br><span class="line">-DWITH_ARCHIVE_STORAGE_ENGINE=1 \</span><br><span class="line">-DWITH_BLACKHOLE_STORAGE_ENGINE=1 \</span><br><span class="line">-DMYSQL_DATADIR=/data/mysql/data \</span><br><span class="line">-DMYSQL_TCP_PORT=3306 \</span><br><span class="line">-DENABLE_DOWNLOADS=1</span><br><span class="line">如果报错找不到CMakeCache.txt则说明没安装ncurses-devel</span><br><span class="line"> </span><br><span class="line"># make &amp;&amp; make install</span><br></pre></td></tr></table></figure><h5 id="修改目录权限"><a href="#修改目录权限" class="headerlink" title="修改目录权限"></a>修改目录权限</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># chmod +w /data/mysql/</span><br><span class="line"># chown -R mysql:mysql /data/mysql/</span><br><span class="line"># ln -s /data/mysql/lib/libmysqlclient.so.18 /usr/lib/libmysqlclient.so.18</span><br><span class="line"># ln -s /data/mysql/mysql.sock /tmp/mysql.sock</span><br></pre></td></tr></table></figure><h5 id="初始化数据库"><a href="#初始化数据库" class="headerlink" title="初始化数据库"></a>初始化数据库</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># cp /data/mysql/support-files/my-default.cnf /etc/my.cnf</span><br><span class="line"># cp /data/mysql/support-files/mysql.server /etc/init.d/mysqld</span><br><span class="line"># /data/mysql/scripts/mysql_install_db --user=mysql --defaults-file=/etc/my.cnf --basedir=/data/mysql --datadir=/data/mysql/data</span><br></pre></td></tr></table></figure><h5 id="启动MySQL服务"><a href="#启动MySQL服务" class="headerlink" title="启动MySQL服务"></a>启动MySQL服务</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># chmod +x /etc/init.d/mysqld</span><br><span class="line"># service mysqld start</span><br><span class="line">#ln –s /data/mysql/bin/mysql /usr/bin/</span><br></pre></td></tr></table></figure><h5 id="初始化密码"><a href="#初始化密码" class="headerlink" title="初始化密码"></a>初始化密码</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#mysql -uroot  -h127.0.0.1 -p</span><br><span class="line">mysql&gt; SET PASSWORD = PASSWORD(&apos;123456&apos;);</span><br></pre></td></tr></table></figure><h4 id="创建Hive用户"><a href="#创建Hive用户" class="headerlink" title="创建Hive用户"></a>创建Hive用户</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;CREATE USER &apos;hive&apos; IDENTIFIED BY &apos;hive&apos;;</span><br><span class="line">mysql&gt;GRANT ALL PRIVILEGES ON *.* TO &apos;hive&apos;@&apos;hadoop-master&apos; WITH GRANT OPTION;</span><br><span class="line">mysql&gt;flush privileges;</span><br></pre></td></tr></table></figure><h4 id="Hive用户登录"><a href="#Hive用户登录" class="headerlink" title="Hive用户登录"></a>Hive用户登录</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-master ~]mysql -h hadoop-master -uhive</span><br><span class="line">mysql&gt;set password = password(&apos;hive&apos;);</span><br></pre></td></tr></table></figure><h4 id="创建Hive数据库"><a href="#创建Hive数据库" class="headerlink" title="创建Hive数据库"></a>创建Hive数据库</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;create database hive;</span><br></pre></td></tr></table></figure><h3 id="配置Hive"><a href="#配置Hive" class="headerlink" title="配置Hive"></a>配置Hive</h3><h4 id="修改配置文件"><a href="#修改配置文件" class="headerlink" title="修改配置文件"></a>修改配置文件</h4><p>进入到hive的配置文件目录下，找到hive-default.xml.template，cp份为hive-default.xml<br>另创建hive-site.xml并添加参数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-master conf]$ pwd</span><br><span class="line">/home/hadoop/apache-hive-1.2.1-bin/conf</span><br><span class="line">[hadoop@hadoop-master conf]$ vi hive-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;jdbc:mysql://hadoop-master:3306/hive?createDatabaseIfNotExist=true&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt;    </span><br><span class="line">&lt;/property&gt;   </span><br><span class="line">&lt;property&gt; </span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; </span><br><span class="line">        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; </span><br><span class="line">        &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;     </span><br><span class="line">&lt;/property&gt;               </span><br><span class="line"> </span><br><span class="line">&lt;property&gt; </span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hive&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;username to use against metastore database&lt;/description&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;property&gt;  </span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;hive&lt;/value&gt;</span><br><span class="line">        &lt;description&gt;password to use against metastore database&lt;/description&gt;  </span><br><span class="line">&lt;/property&gt;          </span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="JDBC下载"><a href="#JDBC下载" class="headerlink" title="JDBC下载"></a>JDBC下载</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-master ~]$ wget http://cdn.mysql.com/Downloads/Connector-J/mysql-connector-java-5.1.36.tar.gz</span><br><span class="line">[hadoop@hadoop-master ~]$ ls</span><br><span class="line">apache-hive-1.2.1-bin  dfs  hadoop-2.7.1  Hsource  tmp</span><br><span class="line">[hadoop@hadoop-master ~]$ cp mysql-connector-java-5.1.33-bin.jar apache-hive-1.2.1-bin/lib/</span><br></pre></td></tr></table></figure><h4 id="Hive客户端配置"><a href="#Hive客户端配置" class="headerlink" title="Hive客户端配置"></a>Hive客户端配置</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-master ~]$ scp -r apache-hive-1.2.1-bin/ hadoop@hadoop-slave:/home/hadoop</span><br><span class="line">[hadoop@hadoop-slave conf]$ vi hive-site.xml</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;property&gt;  </span><br><span class="line">&lt;name&gt;hive.metastore.uris&lt;/name&gt;  </span><br><span class="line">&lt;value&gt;thrift://hadoop-master:9083&lt;/value&gt;  </span><br><span class="line">&lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><h4 id="Hive服务器端访问"><a href="#Hive服务器端访问" class="headerlink" title="Hive服务器端访问"></a>Hive服务器端访问</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-master ~]$ hive</span><br><span class="line">Logging initialized using configuration in jar:file:/home/hadoop/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties</span><br><span class="line">hive&gt; show databases;</span><br><span class="line">OK</span><br><span class="line">default</span><br><span class="line">src</span><br><span class="line">Time taken: 1.332 seconds, Fetched: 2 row(s)</span><br><span class="line">hive&gt; use src;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.037 seconds</span><br><span class="line">hive&gt; create table test1(id int);</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.572 seconds</span><br><span class="line">hive&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">abc</span><br><span class="line">test</span><br><span class="line">test1</span><br><span class="line">Time taken: 0.057 seconds, Fetched: 3 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><p>好了，测试完毕，已经安装成功了。</p><h3 id="安装问题纠错"><a href="#安装问题纠错" class="headerlink" title="安装问题纠错"></a>安装问题纠错</h3><h4 id="Hive数据库编码问题"><a href="#Hive数据库编码问题" class="headerlink" title="Hive数据库编码问题"></a>Hive数据库编码问题</h4><p>错误描述：hive进入后可以创建数据库，但是无法创建表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;create table table_test(id string,name string);</span><br><span class="line">FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask.MetaException(message:javax.jdo.JDODataStoreException: An exception was thrown while adding/validating class(es) : Specified key was too long; max key length is 767 bytes</span><br></pre></td></tr></table></figure><p>解决办法：登录mysql修改下hive数据库的编码方式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql&gt;alter database hive character set latin1;</span><br></pre></td></tr></table></figure><h4 id="防火墙问题"><a href="#防火墙问题" class="headerlink" title="防火墙问题"></a>防火墙问题</h4><p>Hive服务器开启了iptables服务，hive本机可以访问hive服务，hive的客户端hadoop-slave访问报错</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop-slave conf]$ hive</span><br><span class="line">Logging initialized using configuration in jar:file:/home/hadoop/apache-hive-1.2.1-bin/lib/hive-common-1.2.1.jar!/hive-log4j.properties</span><br><span class="line">Exception in thread &quot;main&quot; java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</span><br><span class="line">        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)</span><br><span class="line">        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)</span><br><span class="line">        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)</span><br><span class="line">        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">        at java.lang.reflect.Method.invoke(Method.java:483)</span><br><span class="line">        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)</span><br><span class="line">        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</span><br><span class="line">Caused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient</span><br><span class="line">        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:86)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)</span><br><span class="line">        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)</span><br><span class="line">        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)</span><br><span class="line">        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)</span><br><span class="line">        ... 8 more</span><br><span class="line">Caused by: java.lang.reflect.InvocationTargetException</span><br><span class="line">        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)</span><br><span class="line">        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">        at java.lang.reflect.Constructor.newInstance(Constructor.java:408)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)</span><br><span class="line">        ... 14 more</span><br><span class="line">Caused by: MetaException(message:Could not connect to meta store using any of the URIs provided. Most recent failure: org.apache.thrift.transport.TTransportException: java.net.NoRouteToHostException: No route to host</span><br><span class="line">        at org.apache.thrift.transport.TSocket.open(TSocket.java:187)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:420)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:236)</span><br><span class="line">        at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:74)</span><br><span class="line">        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)</span><br><span class="line">        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">        at java.lang.reflect.Constructor.newInstance(Constructor.java:408)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.&lt;init&gt;(RetryingMetaStoreClient.java:86)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:132)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:104)</span><br><span class="line">        at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3005)</span><br><span class="line">        at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3024)</span><br><span class="line">        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)</span><br><span class="line">        at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:677)</span><br><span class="line">        at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:621)</span><br><span class="line">        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">        at java.lang.reflect.Method.invoke(Method.java:483)</span><br><span class="line">        at org.apache.hadoop.util.RunJar.run(RunJar.java:221)</span><br><span class="line">        at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</span><br><span class="line">Caused by: java.net.NoRouteToHostException: No route to host</span><br><span class="line">        at java.net.PlainSocketImpl.socketConnect(Native Method)</span><br><span class="line">        at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:345)</span><br><span class="line">        at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)</span><br><span class="line">        at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)</span><br><span class="line">        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)</span><br><span class="line">        at java.net.Socket.connect(Socket.java:589)</span><br><span class="line">        at org.apache.thrift.transport.TSocket.open(TSocket.java:182)</span><br><span class="line">        ... 22 more</span><br><span class="line">)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.open(HiveMetaStoreClient.java:466)</span><br><span class="line">        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.&lt;init&gt;(HiveMetaStoreClient.java:236)</span><br><span class="line">        at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.&lt;init&gt;(SessionHiveMetaStoreClient.java:74)</span><br><span class="line">        ... 19 more</span><br></pre></td></tr></table></figure><p>解决办法：比较粗暴直接关掉了防火墙</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@hadoop-master hadoop]# service iptables stop</span><br><span class="line">iptables: Flushing firewall rules: [  OK  ]</span><br><span class="line">iptables: Setting chains to policy ACCEPT: filter [  OK  ]</span><br><span class="line">iptables: Unloading modules: [  OK  ]</span><br><span class="line">[root@hadoop-master hadoop]#</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Hive是基于Hadoop构建的一套数据仓库分析系统，它提供了丰富的SQL查询方式来分析存储在Hadoop 分布式文件系统中的数据。其在Hadoop的架构体系中承担了一个SQL解析的过程，它提供了对外的入口来获取用户的指令然后对指令进行分析，解析出一个MapReduce程序
      
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
      <category term="Hadoop" scheme="http://yoursite.com/tags/Hadoop/"/>
    
      <category term="Hive" scheme="http://yoursite.com/tags/Hive/"/>
    
  </entry>
  
  <entry>
    <title>IDEA部署hadoop运行环境</title>
    <link href="http://yoursite.com/2018/03/06/IDEA%E9%83%A8%E7%BD%B2hadoop%E8%BF%90%E8%A1%8C%E7%8E%AF%E5%A2%83/"/>
    <id>http://yoursite.com/2018/03/06/IDEA部署hadoop运行环境/</id>
    <published>2018-03-06T03:50:35.000Z</published>
    <updated>2018-03-06T06:52:49.620Z</updated>
    
    <content type="html"><![CDATA[<h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>​       前段时间配置好了基于mac的hadoop完全分布式环境，一直想着怎么样去用编译器写程序然后直接在hadoop环境中运行呢，经过一番摸索，写下此文章分享交流。</p><h1 id="环境介绍"><a href="#环境介绍" class="headerlink" title="环境介绍"></a>环境介绍</h1><blockquote><ul><li>os x EI Capitan 10.11.5</li><li>虚拟机：Parallels Desktop</li><li>ubuntukylin 14.04 64bit * 2</li><li>hadoop 2.6.2</li><li>os x上的jdk版本：1.8.0_73，ubuntu上的jdk版本：1.8.0_91（不同机器上的jdk版本不要求一样）</li><li>IntelliJ IDEA 16.1</li></ul></blockquote><h4 id="如果没有配置好hadoop的环境，请参考本人另一篇博文：http-blog-csdn-net-wk51920-article-details-51686038"><a href="#如果没有配置好hadoop的环境，请参考本人另一篇博文：http-blog-csdn-net-wk51920-article-details-51686038" class="headerlink" title="如果没有配置好hadoop的环境，请参考本人另一篇博文：http://blog.csdn.net/wk51920/article/details/51686038"></a>如果没有配置好hadoop的环境，请参考本人另一篇博文：<a href="http://blog.csdn.net/wk51920/article/details/51686038" target="_blank" rel="noopener">http://blog.csdn.net/wk51920/article/details/51686038</a></h4><h1 id="下载及安装IntelliJ-IDEA-16-1"><a href="#下载及安装IntelliJ-IDEA-16-1" class="headerlink" title="下载及安装IntelliJ IDEA 16.1"></a>下载及安装IntelliJ IDEA 16.1</h1><p>​       1. 登录官网：<a href="https://www.jetbrains.com/idea/" target="_blank" rel="noopener">https://www.jetbrains.com/idea/</a>下载最新版本的编译器。<br>​       2. 安装编译器。</p><h1 id="创建hadoop工程"><a href="#创建hadoop工程" class="headerlink" title="创建hadoop工程"></a>创建hadoop工程</h1><p>​       1. 打开IntelliJ IDEA编译器，创建一个新的工程，点击next<br><img src="http://img.blog.csdn.net/20160615232457648" alt="这里写图片描述"><br>​       2. 输入工程的名字，点击Finish<br><img src="http://img.blog.csdn.net/20160615232731524" alt="这里写图片描述"><br>​       3. 创建如下图的包结构及WordCount.java文件：<br><img src="http://img.blog.csdn.net/20160615233423402" alt="这里写图片描述"><br>​       4. 添加JDK，如果已经配置了JDK可以省略此步骤：在侧边栏右击<img src="http://img.blog.csdn.net/20160615233517597" alt="这里写图片描述">选择“Open Module Settings”<br><img src="http://img.blog.csdn.net/20160615233741840" alt="这里写图片描述"><br>​       点击“news”选择jdk的根目录即可。<br><img src="http://img.blog.csdn.net/20160615234006369" alt="这里写图片描述"><br>​       5. 添加hadoop各种jar包<br>​       打开“Module Settings”点击左侧边栏中的Libraries。<br><img src="http://img.blog.csdn.net/20160615234327296" alt="这里写图片描述"><br>​       点击右侧的“+”号，并选择“Java”，在路径中选择hadoop的各种jar包，我的hadoop jar包路劲为：/usr/hadoop/hadoop-2.6.2/share/hadoop下的所有目录中的jar包。另外还需要特别将/usr/hadoop/hadoop-2.6.2/share/hadoop/common/lib下的所有jar包添加进工程，并点击“Apply”或“OK”。<br><img src="http://img.blog.csdn.net/20160615234555359" alt="这里写图片描述"><br><img src="http://img.blog.csdn.net/20160615234949085" alt="这里写图片描述"></p><h1 id="配置编译环境"><a href="#配置编译环境" class="headerlink" title="配置编译环境"></a>配置编译环境</h1><p>​       1. 点击右上角的<img src="http://img.blog.csdn.net/20160615235415050" alt="这里写图片描述">中的下拉箭头，点击“Edit Configurations…”，并点击左上角的“+”号，并选择“Application”<br><img src="http://img.blog.csdn.net/20160615235725364" alt="这里写图片描述"><br>​       2. 在“Main class”中填写我们需要运行的程序：com.wk51920.WordCount;<br>​        在“Program arguments”中填写程序需要的“输入文件”和“输出文件”。在此程序中，“输入文件”即需要用WordCount去统计单词的源文件的位置，“输出文件”即保存统计结果的文件。此栏的完整内容为：hdfs://master:8020/input/README.txt hdfs://master:8020/output/。这两个路径根据你配置的hadoop环境相关，即core-site.xml配置文件中的fs.defaultFS属性的值，即是HDFS系统的根目录。<br>​       在“Name”处改为：hadoop,点击“Apply”。<br><img src="http://img.blog.csdn.net/20160616000927594" alt="这里写图片描述"></p><h1 id="填写WordCount代码"><a href="#填写WordCount代码" class="headerlink" title="填写WordCount代码"></a>填写WordCount代码</h1><p>​       选择WordCount文件，写入如下代码：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">package com.wk51920;</span><br><span class="line"></span><br><span class="line">import org.apache.hadoop.fs.Path;</span><br><span class="line">import org.apache.hadoop.io.IntWritable;</span><br><span class="line">import org.apache.hadoop.io.LongWritable;</span><br><span class="line">import org.apache.hadoop.io.Text;</span><br><span class="line">import org.apache.hadoop.mapred.*;</span><br><span class="line"></span><br><span class="line">import java.io.IOException;</span><br><span class="line">import java.util.Iterator;</span><br><span class="line">import java.util.StringTokenizer;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line"> * Created by wk51920 on 16/6/3.</span><br><span class="line"> */</span><br><span class="line">public class WordCount  &#123;</span><br><span class="line">    public static class Map extends MapReduceBase implements Mapper&lt;LongWritable, Text, Text, IntWritable&gt;&#123;</span><br><span class="line">        private final static IntWritable one = new IntWritable(1);</span><br><span class="line">        private Text word = new Text();</span><br><span class="line">        @Override</span><br><span class="line">        public void map(LongWritable longWritable, Text text, OutputCollector&lt;Text, IntWritable&gt; outputCollector, Reporter reporter) throws IOException &#123;</span><br><span class="line">            String line = text.toString();</span><br><span class="line">            StringTokenizer tokenizer = new StringTokenizer(line);</span><br><span class="line">            while(tokenizer.hasMoreTokens())&#123;</span><br><span class="line">                word.set(tokenizer.nextToken());</span><br><span class="line">                outputCollector.collect(word,one);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static class Reduce extends MapReduceBase implements Reducer&lt;Text,IntWritable,Text, IntWritable&gt;&#123;</span><br><span class="line">        @Override</span><br><span class="line">        public void reduce(Text text, Iterator&lt;IntWritable&gt; iterator, OutputCollector&lt;Text, IntWritable&gt; outputCollector, Reporter reporter) throws IOException &#123;</span><br><span class="line">            int sum= 0;</span><br><span class="line">            while(iterator.hasNext())&#123;</span><br><span class="line">                sum += iterator.next().get();</span><br><span class="line">            &#125;</span><br><span class="line">            outputCollector.collect(text, new IntWritable(sum));</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) throws Exception&#123;</span><br><span class="line">        JobConf conf = new JobConf(WordCount.class);</span><br><span class="line">        conf.setJobName(&quot;wordCount&quot;);</span><br><span class="line">        conf.setOutputKeyClass(Text.class);</span><br><span class="line">        conf.setOutputValueClass(IntWritable.class);</span><br><span class="line"></span><br><span class="line">        conf.setMapperClass(Map.class);</span><br><span class="line">        conf.setReducerClass(Reduce.class);</span><br><span class="line"></span><br><span class="line">        conf.setInputFormat(TextInputFormat.class);</span><br><span class="line">        conf.setOutputFormat(TextOutputFormat.class);</span><br><span class="line"></span><br><span class="line">        FileInputFormat.setInputPaths(conf,new Path(args[0]));</span><br><span class="line">        FileOutputFormat.setOutputPath(conf, new Path(args[1]));</span><br><span class="line"></span><br><span class="line">        JobClient.runJob(conf);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061</span><br></pre></td></tr></table></figure><h1 id="将文件放入HDFS系统"><a href="#将文件放入HDFS系统" class="headerlink" title="将文件放入HDFS系统"></a>将文件放入HDFS系统</h1><p>​       如果没有启动hadoop服务，先启动服务。</p><p>​       1. 在hdfs系统中建立input文件夹：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir /input  1</span><br></pre></td></tr></table></figure><p>​       2. 将本地的README.txt放入HDFS中的input文件夹下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -copyFromLocal README.txt /input1</span><br></pre></td></tr></table></figure><h3 id="注意运行此程序前，在HDFS系统上不能存在output文件夹，如果存在请删除再运行程序。"><a href="#注意运行此程序前，在HDFS系统上不能存在output文件夹，如果存在请删除再运行程序。" class="headerlink" title="注意运行此程序前，在HDFS系统上不能存在output文件夹，如果存在请删除再运行程序。"></a>注意运行此程序前，在HDFS系统上不能存在output文件夹，如果存在请删除再运行程序。</h3><h1 id="运行WordCount程序"><a href="#运行WordCount程序" class="headerlink" title="运行WordCount程序"></a>运行WordCount程序</h1><p>​        选择WordCount文件，并在代码区右击，选择“Run ‘hadoop’”<br><img src="http://img.blog.csdn.net/20160616002221754" alt="这里写图片描述"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h1&gt;&lt;p&gt;​       前段时间配置好了基于mac的hadoop完全分布式环境，一直想着怎么样去用编译器写程序然后直接在hadoop环境中运行呢，经
      
    
    </summary>
    
      <category term="Hadoop" scheme="http://yoursite.com/categories/Hadoop/"/>
    
    
      <category term="Hadoop" scheme="http://yoursite.com/tags/Hadoop/"/>
    
      <category term="IDEA" scheme="http://yoursite.com/tags/IDEA/"/>
    
  </entry>
  
  <entry>
    <title>Java动态代理</title>
    <link href="http://yoursite.com/2018/03/06/Java%E5%8A%A8%E6%80%81%E4%BB%A3%E7%90%86/"/>
    <id>http://yoursite.com/2018/03/06/Java动态代理/</id>
    <published>2018-03-06T03:50:35.000Z</published>
    <updated>2018-03-06T06:54:41.214Z</updated>
    
    <content type="html"><![CDATA[<h2 id="业务场景"><a href="#业务场景" class="headerlink" title="业务场景"></a>业务场景</h2><ol><li>旧业务<ul><li>买家调用action，购买衣服，衣服在数据库的标价为50元，购买流程就是简单的调用。</li></ul></li><li>新业务<ul><li>在原先的价格上可以使用优惠券，但是这个功能在以前没有实现过，我们通过代理类，代理了原先的接口方法，在这个方法的基础上，修改了返回值。<img src="http://ou9crezlk.bkt.clouddn.com/blog/170930/b0AHh2bH35.png?imageslim" alt="mark"></li></ul></li></ol><h2 id="代理实现流程"><a href="#代理实现流程" class="headerlink" title="代理实现流程"></a>代理实现流程</h2><ol><li>书写代理类和代理方法，在代理方法中实现代理Proxy.newProxyInstance</li><li>代理中需要的参数分别为：被代理的类的类加载器soneObjectclass.getClassLoader()，被代理类的所有实现接口new Class[] { Interface.class }，句柄方法new InvocationHandler()</li><li>在句柄方法中复写invoke方法，invoke方法的输入有3个参数Object proxy（代理类对象）, Method method（被代理类的方法）,Object[] args（被代理类方法的传入参数），在这个方法中，我们可以定制化的开发新的业务。</li><li>获取代理类，强转成被代理的接口</li><li>最后，我们可以像没被代理一样，调用接口的认可方法，方法被调用后，方法名和参数列表将被传入代理类的invoke方法中，进行新业务的逻辑流程。</li></ol><h3 id="原业务接口IBoss"><a href="#原业务接口IBoss" class="headerlink" title="原业务接口IBoss"></a>原业务接口IBoss</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">IBoss</span> </span>&#123;<span class="comment">//接口</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">yifu</span><span class="params">(String size)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="原业务实现类"><a href="#原业务实现类" class="headerlink" title="原业务实现类"></a>原业务实现类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Boss</span> <span class="keyword">implements</span> <span class="title">IBoss</span></span>&#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">yifu</span><span class="params">(String size)</span></span>&#123;</span><br><span class="line">System.err.println(<span class="string">"天猫小强旗舰店，老板给客户发快递----衣服型号："</span>+size);</span><br><span class="line"><span class="comment">//这件衣服的价钱，从数据库读取</span></span><br><span class="line"><span class="keyword">return</span> <span class="number">50</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">kuzi</span><span class="params">()</span></span>&#123;</span><br><span class="line">System.err.println(<span class="string">"天猫小强旗舰店，老板给客户发快递----裤子"</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="原业务调用"><a href="#原业务调用" class="headerlink" title="原业务调用"></a>原业务调用</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">SaleAction</span> </span>&#123;</span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">saleByBossSelf</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">IBoss boss = <span class="keyword">new</span> Boss();</span><br><span class="line">System.out.println(<span class="string">"老板自营！"</span>);</span><br><span class="line"><span class="keyword">int</span> money = boss.yifu(<span class="string">"xxl"</span>);</span><br><span class="line">System.out.println(<span class="string">"衣服成交价："</span> + money);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="代理类"><a href="#代理类" class="headerlink" title="代理类"></a>代理类</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> IBoss <span class="title">getProxyBoss</span><span class="params">(<span class="keyword">final</span> <span class="keyword">int</span> discountCoupon)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Object proxedObj = Proxy.newProxyInstance(Boss.class.getClassLoader(),</span><br><span class="line"><span class="keyword">new</span> Class[] &#123; IBoss.class &#125;, <span class="keyword">new</span> InvocationHandler() &#123;</span><br><span class="line"><span class="function"><span class="keyword">public</span> Object <span class="title">invoke</span><span class="params">(Object proxy, Method method,</span></span></span><br><span class="line"><span class="function"><span class="params">Object[] args)</span> <span class="keyword">throws</span> Throwable </span>&#123;</span><br><span class="line">Integer returnValue = (Integer) method.invoke(<span class="keyword">new</span> Boss(),</span><br><span class="line">args);<span class="comment">// 调用原始对象以后返回的值</span></span><br><span class="line"><span class="keyword">return</span> returnValue - discountCoupon;</span><br><span class="line">&#125;</span><br><span class="line">&#125;);</span><br><span class="line"><span class="keyword">return</span> (IBoss)proxedObj;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="新业务调用"><a href="#新业务调用" class="headerlink" title="新业务调用"></a>新业务调用</h3><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProxySaleAction</span> </span>&#123;</span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">saleByProxy</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">IBoss boss = ProxyBoss.getProxyBoss(<span class="number">20</span>);<span class="comment">// 将代理的方法实例化成接口</span></span><br><span class="line">System.out.println(<span class="string">"代理经营！"</span>);</span><br><span class="line"><span class="keyword">int</span> money = boss.yifu(<span class="string">"xxl"</span>);<span class="comment">// 调用接口的方法，实际上调用方式没有变</span></span><br><span class="line">System.out.println(<span class="string">"衣服成交价："</span> + money);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;业务场景&quot;&gt;&lt;a href=&quot;#业务场景&quot; class=&quot;headerlink&quot; title=&quot;业务场景&quot;&gt;&lt;/a&gt;业务场景&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;旧业务&lt;ul&gt;
&lt;li&gt;买家调用action，购买衣服，衣服在数据库的标价为50元，购买流程就是简单的调用。&lt;/
      
    
    </summary>
    
      <category term="Java" scheme="http://yoursite.com/categories/Java/"/>
    
    
      <category term="Java" scheme="http://yoursite.com/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>HDFS学习</title>
    <link href="http://yoursite.com/2018/03/06/HDFS%E5%AD%A6%E4%B9%A0/"/>
    <id>http://yoursite.com/2018/03/06/HDFS学习/</id>
    <published>2018-03-06T03:50:35.000Z</published>
    <updated>2018-03-06T07:33:12.122Z</updated>
    
    <content type="html"><![CDATA[<h2 id="HDFS前言"><a href="#HDFS前言" class="headerlink" title="HDFS前言"></a>HDFS前言</h2><ul><li>设计思想<ul><li>分而治之：将大文件、大批量文件，分布式存放在大量服务器上，<strong>以便于采取分而治之的方式对海量数据进行运算分析</strong>； </li></ul></li></ul><ul><li>在大数据系统中作用：<ul><li>为各类分布式运算框架（如：mapreduce，spark，tez，……）提供数据存储服务</li></ul></li><li>重点概念：文件切块，副本存放，元数据</li></ul><h2 id="HDFS的概念和特性"><a href="#HDFS的概念和特性" class="headerlink" title="HDFS的概念和特性"></a>HDFS的概念和特性</h2><ul><li><strong>首先，它是一个文件系统</strong>，用于存储文件，通过统一的命名空间——目录树来定位文件</li><li><strong>其次，它是分布式的</strong>，由很多服务器联合起来实现其功能，集群中的服务器有各自的角色；</li><li><p><strong>重要特性如下：</strong></p><ol><li>HDFS中的文件在物理上是<strong>分块存储（block）</strong>，块的大小可以通过配置参数( dfs.blocksize)来规定，默认大小在hadoop2.x版本中是128M，老版本中是64M</li><li>HDFS文件系统会给客户端提供一个<strong>统一的抽象目录树</strong>，客户端通过路径来访问文件，形如：hdfs://namenode:port/dir-a/dir-b/dir-c/file.data</li><li><strong>目录结构及文件分块信息(元数据)</strong>的管理由namenode节点承担——namenode是HDFS集群主节点，负责维护整个hdfs文件系统的目录树，以及每一个路径（文件）所对应的block块信息（block的id，及所在的datanode服务器）</li><li>文件的各个block的存储管理由datanode节点承担—- datanode是HDFS集群从节点，每一个block都可以在多个datanode上存储多个副本（副本数量也可以通过参数设置dfs.replication）</li><li>HDFS是设计成适应一次写入，多次读出的场景，且不支持文件的修改</li></ol><p><strong>(注：适合用来做数据分析，并不适合用来做网盘应用，因为，不便修改，延迟大，网络开销大，成本太高)</strong></p></li></ul><h2 id="HDFS的shell-命令行客户端-操作"><a href="#HDFS的shell-命令行客户端-操作" class="headerlink" title="HDFS的shell(命令行客户端)操作"></a>HDFS的shell(命令行客户端)操作</h2><h3 id="HDFS命令行客户端使用操作"><a href="#HDFS命令行客户端使用操作" class="headerlink" title="HDFS命令行客户端使用操作"></a>HDFS命令行客户端使用操作</h3><ul><li><code>hadoop fs -ls /</code></li></ul><h3 id="常用命令参数介绍"><a href="#常用命令参数介绍" class="headerlink" title="常用命令参数介绍"></a>常用命令参数介绍</h3><table><thead><tr><th style="text-align:left">命令</th><th>功能</th><th>示例</th></tr></thead><tbody><tr><td style="text-align:left">-help</td><td>输出这个命令参数手册</td><td></td></tr><tr><td style="text-align:left">-ls</td><td>显示目录信息</td><td><code>hadoop fs -ls /</code></td></tr><tr><td style="text-align:left">-mkdir</td><td>在hdfs上创建目录</td><td><code>hadoop fs  -mkdir  -p  /aaa/bbb/cc/dd</code></td></tr><tr><td style="text-align:left">-moveFromLocal</td><td>从本地剪切粘贴到hdfs</td><td><code>hadoop fs -moveFromLocal /home/hadoop/a.txt  /aaa/bbb/cc/dd</code></td></tr><tr><td style="text-align:left">-moveToLocal</td><td>从hdfs剪切粘贴到本地</td><td><code>hadoop fs -moveToLocal  /aaa/bbb/cc/dd  /home/hadoop/a.txt</code></td></tr><tr><td style="text-align:left">-appendToFile</td><td>追加一个文件到已经存在的文件末尾</td><td><code>Hadoop  fs  -appendToFile  ./hello.txt /hello.txt</code></td></tr><tr><td style="text-align:left">-cat</td><td>显示文件内容</td><td><code>hadoop fs -cat  /hello.txt</code></td></tr><tr><td style="text-align:left">-tail</td><td>显示一个文件的末尾</td><td><code>hadoop  fs  -tail  /weblog/access_log.1</code></td></tr><tr><td style="text-align:left">-text</td><td>以字符形式打印一个文件的内容</td><td><code>hadoop  fs  -text  /weblog/access_log.1</code></td></tr><tr><td style="text-align:left">-chgrp</td><td>linux文件系统中的用法一样，对文件所属权限</td><td></td></tr><tr><td style="text-align:left">-chmod</td><td>linux文件系统中的用法一样，对文件所属权限</td><td><code>hadoop  fs  -chmod  666  /hello.txt</code></td></tr><tr><td style="text-align:left">-chown</td><td>linux文件系统中的用法一样，对文件所属权限</td><td><code>hadoop  fs  -chown  someuser:somegrp   /hello.txt</code></td></tr><tr><td style="text-align:left">-copyFromLocal</td><td>从本地文件系统中拷贝文件到hdfs路径去</td><td><code>hadoop  fs  -copyFromLocal  ./jdk.tar.gz  /aaa/</code></td></tr><tr><td style="text-align:left">-copyToLocal</td><td>从hdfs拷贝到本地</td><td><code>hadoop fs -copyToLocal /aaa/jdk.tar.gz</code></td></tr><tr><td style="text-align:left">-cp</td><td>从hdfs的一个路径拷贝hdfs的另一个路径</td><td><code>hadoop  fs  -cp  /aaa/jdk.tar.gz  /bbb/jdk.tar.gz.2</code></td></tr><tr><td style="text-align:left">-mv</td><td>在hdfs目录中移动文件</td><td><code>hadoop  fs  -mv  /aaa/jdk.tar.gz  /</code></td></tr><tr><td style="text-align:left">-get</td><td>等同于copyToLocal，就是从hdfs下载文件到本地</td><td><code>hadoop fs -get  /aaa/jdk.tar.gz</code></td></tr><tr><td style="text-align:left">-getmerge</td><td>合并下载多个文件</td><td><code>hadoop fs -getmerge /aaa/log.* ./log.sum</code></td></tr><tr><td style="text-align:left">-put</td><td>等同于copyFromLocal</td><td><code>hadoop  fs  -put  /aaa/jdk.tar.gz  /bbb/jdk.tar.gz.2</code></td></tr><tr><td style="text-align:left">-rm</td><td>删除文件或文件夹</td><td><code>hadoop fs -rm -r /aaa/bbb/</code></td></tr><tr><td style="text-align:left">-rmdir</td><td>删除空目录</td><td><code>hadoop  fs  -rmdir   /aaa/bbb/ccc</code></td></tr><tr><td style="text-align:left">-df</td><td>统计文件系统的可用空间信息</td><td><code>hadoop  fs  -df  -h  /</code></td></tr><tr><td style="text-align:left">-du</td><td>统计文件夹的大小信息</td><td><code>hadoop  fs  -du  -s  -h /aaa/*</code></td></tr><tr><td style="text-align:left">-count</td><td>统计一个指定目录下的文件节点数量</td><td><code>hadoop fs -count /aaa/</code></td></tr><tr><td style="text-align:left">-setrep</td><td>设置hdfs中文件的副本数量</td><td>`hadoop fs -setrep 3 /aaa/jdk.tar.gz</td></tr></tbody></table><h2 id="HDFS原理篇"><a href="#HDFS原理篇" class="headerlink" title="HDFS原理篇"></a>HDFS原理篇</h2><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><ol><li>HDFS集群分为两大角色：NameNode、DataNode  (Secondary Namenode)</li><li>NameNode负责管理整个文件系统的元数据</li><li>DataNode 负责管理用户的文件数据块</li><li>文件会按照固定的大小（blocksize）切成若干块后分布式存储在若干台datanode上</li><li>每一个文件块可以有多个副本，并存放在不同的datanode上</li><li>Datanode会定期向Namenode汇报自身所保存的文件block信息，而namenode则会负责保持文件的副本数量</li><li>HDFS的内部工作机制对客户端保持透明，客户端请求访问HDFS都是通过向namenode申请来进行</li></ol><h3 id="HDFS写数据流程"><a href="#HDFS写数据流程" class="headerlink" title="HDFS写数据流程"></a>HDFS写数据流程</h3><ul><li>客户端要向HDFS写数据，首先要跟namenode通信以确认可以写文件并获得接收文件block的datanode，然后，客户端按顺序将文件逐个block传递给相应datanode，并由接收到block的datanode负责向其他datanode复制block的副本</li></ul><p>####详细步骤图</p><p>  ​</p><ul><li><img src="http://ou9crezlk.bkt.clouddn.com/blog/171023/Gi2l8jkCh6.png?imageslim" alt="mark"></li></ul><p>####详细步骤解析</p><ol><li>根namenode通信请求上传文件，namenode检查目标文件是否已存在，父目录是否存在</li><li>namenode返回是否可以上传</li><li>client请求第一个 block该传输到哪些datanode服务器上</li><li>namenode返回3个datanode服务器ABC</li><li>client请求3台dn中的一台A上传数据（本质上是一个RPC调用，建立pipeline），A收到请求会继续调用B，然后B调用C，将真个pipeline建立完成，逐级返回客户端</li><li>client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答</li><li>当一个block传输完成之后，client再次请求namenode上传第二个block的服务器。</li></ol><h3 id="HDFS读数据流程"><a href="#HDFS读数据流程" class="headerlink" title="HDFS读数据流程"></a>HDFS读数据流程</h3><ul><li>客户端将要读取的文件路径发送给namenode，namenode获取文件的元信息（主要是block的存放位置信息）返回给客户端，客户端根据返回的信息找到相应datanode逐个获取文件的block并在客户端本地进行数据追加合并从而获得整个文件</li></ul><h4 id="详细步骤图"><a href="#详细步骤图" class="headerlink" title="详细步骤图"></a>详细步骤图</h4><p><img src="http://ou9crezlk.bkt.clouddn.com/blog/171023/9E10iECID7.png?imageslim" alt="mark"></p><h4 id="详细步骤解析"><a href="#详细步骤解析" class="headerlink" title="详细步骤解析"></a>详细步骤解析</h4><ol><li>跟namenode通信查询元数据，找到文件块所在的datanode服务器</li><li>挑选一台datanode（就近原则，然后随机）服务器，请求建立socket流</li><li>datanode开始发送数据（从磁盘里面读取数据放入流，以packet为单位来做校验）</li><li>客户端以packet为单位接收，现在本地缓存，然后写入目标文件</li></ol><h3 id="NAMENODE工作机制"><a href="#NAMENODE工作机制" class="headerlink" title="NAMENODE工作机制"></a>NAMENODE工作机制</h3><p>问题场景：</p><ol><li>集群启动后，可以查看文件，但是上传文件时报错，打开web页面可看到namenode正处于safemode状态，怎么处理？</li><li>Namenode服务器的磁盘故障导致namenode宕机，如何挽救集群及数据？</li><li>Namenode是否可以有多个？namenode内存要配置多大？namenode跟集群数据存储能力有关系吗？</li><li>文件的blocksize究竟调大好还是调小好？</li></ol><h4 id="NAMENODE职责"><a href="#NAMENODE职责" class="headerlink" title="NAMENODE职责"></a>NAMENODE职责</h4><ul><li>NAMENODE职责：<ul><li>负责客户端请求的响应</li><li>元数据的管理（查询，修改）</li></ul></li></ul><h4 id="元数据管理"><a href="#元数据管理" class="headerlink" title="元数据管理"></a>元数据管理</h4><ul><li>namenode对数据的管理采用了三种存储形式：<ul><li>内存元数据(NameSystem)</li><li>磁盘元数据镜像文件</li><li>数据操作日志文件（可通过日志运算出元数据）</li></ul></li></ul><h4 id="元数据存储机制"><a href="#元数据存储机制" class="headerlink" title="元数据存储机制"></a>元数据存储机制</h4><ul><li>内存中有一份完整的元数据(内存meta data)</li><li>磁盘有一个“准完整”的元数据镜像（fsimage）文件(在namenode的工作目录中)</li><li>用于衔接内存metadata和持久化元数据镜像fsimage之间的操作日志（edits文件）注：当客户端对hdfs中的文件进行新增或者修改操作，操作记录首先被记入edits日志文件中，当客户端操作成功后，相应的元数据会更新到内存meta.data中</li></ul><h4 id="元数据手动查看"><a href="#元数据手动查看" class="headerlink" title="元数据手动查看"></a>元数据手动查看</h4><ul><li>可以通过hdfs的一个工具来查看edits中的信息<ul><li>bin/hdfs oev -i edits -o edits.xml</li><li>bin/hdfs oiv -i fsimage_0000000000000000087 -p XML -o fsimage.xml</li></ul></li></ul><h4 id="元数据的checkpoint"><a href="#元数据的checkpoint" class="headerlink" title="元数据的checkpoint"></a>元数据的checkpoint</h4><ul><li>每隔一段时间，会由secondary namenode将namenode上积累的所有edits和一个最新的fsimage下载到本地，并加载到内存进行merge（这个过程称为checkpoint）</li><li>checkpoint的详细过程图</li></ul><p><img src="http://ou9crezlk.bkt.clouddn.com/blog/171023/BaHlDK0493.png?imageslim" alt="mark"></p><ul><li><p>checkpoint操作的触发条件配置参数</p><blockquote><p>dfs.namenode.checkpoint.check.period=60  #检查触发条件是否满足的频率，60秒</p></blockquote><p>dfs.namenode.checkpoint.dir=file://${hadoop.tmp.dir}/dfs/namesecondary</p><p>#以上两个参数做checkpoint操作时，secondary namenode的本地工作目录</p><p>dfs.namenode.checkpoint.edits.dir=${dfs.namenode.checkpoint.dir}</p><p>dfs.namenode.checkpoint.max-retries=3  #最大重试次数<br>dfs.namenode.checkpoint.period=3600  #两次checkpoint之间的时间间隔3600秒<br>dfs.namenode.checkpoint.txns=1000000 #两次checkpoint之间最大的操作记录</p></li><li><p>checkpoint的附带作用</p></li><li><p>namenode和secondary namenode的工作目录存储结构完全相同，所以，当namenode故障退出需要重新恢复时，可以从secondary namenode的工作目录中将fsimage拷贝到namenode的工作目录，以恢复namenode的元数据</p></li></ul><h4 id="元数据目录说明"><a href="#元数据目录说明" class="headerlink" title="元数据目录说明"></a>元数据目录说明</h4><ul><li><p>在第一次部署好Hadoop集群的时候，我们需要在NameNode（NN）节点上格式化磁盘\$HADOOP_HOME/bin/hdfs namenode -format格式化完成之后，将会在$dfs.namenode.name.dir/current目录下如下的文件结构:</p><ul><li><blockquote><p>current/</p><p>|– VERSION</p><p>|– edits_*_</p><p>_|– fsimage_0000000000008547077</p><p>|– fsimage_0000000000008547077.md5</p><p>|– seen_txid</p></blockquote></li></ul></li><li><p>其中的dfs.name.dir是在hdfs-site.xml文件中配置的，默认值如下:</p><ul><li><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>file://$&#123;hadoop.tmp.dir&#125;/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>hadoop.tmp.dir是在core-site.xml中配置的，默认值如下:</p><ul><li><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/tmp/hadoop-$&#123;user.name&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>A base for other temporary directories.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li><li><p>dfs.namenode.name.dir属性可以配置多个目录</p><ul><li>如：/data1/dfs/name,/data2/dfs/name,/data3/dfs/name,….。各个目录存储的文件结构和内容都完全一样，相当于备份，这样做的好处是当其中一个目录损坏了，也不会影响到Hadoop的元数据，特别是当其中一个目录是NFS（网络文件系统Network File System，NFS）之上，即使你这台机器损坏了，元数据也得到保存。</li></ul></li><li><p>下面对$dfs.namenode.name.dir/current/目录下的文件进行解释。</p><ul><li><p>VERSION文件是Java属性文件，内容大致如下：</p></li><li><blockquote><p>#Fri Nov 15 19:47:46 CST 2013</p><p>namespaceID=934548976<br>clusterID=CID-cdff7d73-93cd-4783-9399-0a22e6dce196<br>cTime=0<br>storageType=NAME_NODE<br>blockpoolID=BP-893790215-192.168.24.72-1383809616115<br>layoutVersion=-47</p></blockquote></li><li><p>namespaceID是文件系统的唯一标识符，在文件系统首次格式化之后生成的；</p></li><li><p>storageType说明这个文件存储的是什么进程的数据结构信息（如果是DataNode，storageType=DATA_NODE）；</p></li><li><p>cTime表示NameNode存储时间的创建时间，由于我的NameNode没有更新过，所以这里的记录值为0，以后对NameNode升级之后，cTime将会记录更新时间戳；</p></li><li><p>layoutVersion表示HDFS永久性数据结构的版本信息， 只要数据结构变更，版本号也要递减，此时的HDFS也需要升级，否则磁盘仍旧是使用旧版本的数据结构，这会导致新版本的NameNode无法使用；</p></li><li><p>clusterID是系统生成或手动指定的集群ID，在-clusterid选项中可以使用它；如下说明</p><ul><li>使用如下命令格式化一个Namenode：<ul><li><code>$HADOOP_HOME/bin/hdfs namenode -format [-clusterId &lt;cluster_id&gt;]</code><br>选择一个唯一的cluster_id，并且这个cluster_id不能与环境中其他集群有冲突。如果没有提供cluster_id，则会自动生成一个唯一的ClusterID。</li></ul></li><li>使用如下命令格式化其他Namenode<ul><li><code>$HADOOP_HOME/bin/hdfs namenode -format -clusterId &lt;cluster_id&gt;</code></li></ul></li><li>升级集群至最新版本。在升级过程中需要提供一个ClusterID，例如：<ul><li><code>$HADOOP_CONF_DIR  -upgrade -clusterId &lt;cluster_ID&gt;</code>如果没有提供ClusterID，则会自动生成一个ClusterID。</li></ul></li></ul></li><li><p>blockpoolID：是针对每一个Namespace所对应的blockpool的ID，上面的这个BP-893790215-192.168.24.72-1383809616115就是在我的ns1的namespace下的存储块池的ID，这个ID包括了其对应的NameNode节点的ip地址。</p></li></ul></li><li><p>\$dfs.namenode.name.dir/current/seen_txid非常重要，是存放transactionId的文件，format之后是0，它代表的是namenode里面的edits_*文件的尾数，namenode重启的时候，会按照seen_txid的数字，循序从头跑edits_0000001~到seen_txid的数字。所以当你的hdfs发生异常重启的时候，一定要比对seen_txid内的数字是不是你edits最后的尾数，不然会发生建置namenode时metaData的资料有缺少，导致误删Datanode上多余Block的资讯。</p></li><li><p>$dfs.namenode.name.dir/current目录下在format的同时也会生成fsimage和edits文件，及其对应的md5校验文件。</p></li><li><p>文件中记录的是edits滚动的序号，每次重启namenode时，namenode就知道要将哪些edits进行加载edits</p></li></ul><h3 id="DATANODE的工作机制"><a href="#DATANODE的工作机制" class="headerlink" title="DATANODE的工作机制"></a>DATANODE的工作机制</h3><p>问题场景：</p><ol><li>集群容量不够，怎么扩容？</li><li>如果有一些datanode宕机，该怎么办？</li><li>datanode明明已启动，但是集群中的可用datanode列表中就是没有，怎么办？</li></ol><h4 id="概述-1"><a href="#概述-1" class="headerlink" title="概述"></a>概述</h4><h5 id="Datanode工作职责"><a href="#Datanode工作职责" class="headerlink" title="Datanode工作职责"></a>Datanode工作职责</h5><ul><li><p>存储管理用户的文件块数据</p></li><li><p>定期向namenode汇报自身所持有的block信息（通过心跳信息上报）</p><ul><li><p>（这点很重要，因为，当集群中发生某些block副本失效时，集群如何恢复block初始副本数量的问题）</p></li><li><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blockreport.intervalMsec<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>3600000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>Determines block reporting interval in milliseconds.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul></li></ul><p>#####Datanode掉线判断时限参数</p><p>datanode进程死亡或者网络故障造成datanode无法与namenode通信，namenode不会立即把该节点判定为死亡，要经过一段时间，这段时间暂称作超时时长。HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为：</p><pre><code>timeout  = 2 * heartbeat.recheck.interval + 10 * dfs.heartbeat.interval。而默认的heartbeat.recheck.interval 大小为5分钟，dfs.heartbeat.interval默认为3秒。需要注意的是hdfs-site.xml 配置文件中的heartbeat.recheck.interval的单位为毫秒，dfs.heartbeat.interval的单位为秒。所以，举个例子，如果heartbeat.recheck.interval设置为5000（毫秒），dfs.heartbeat.interval设置为3（秒，默认），则总的超时时间为40秒。</code></pre><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>heartbeat.recheck.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.heartbeat.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>#####观察验证DATANODE功能</p><ul><li>上传一个文件，观察文件的block具体的物理存放情况：</li><li>在每一台datanode机器上的这个目录中能找到文件的切块：<ul><li><code>/home/hadoop/app/hadoop-2.4.1/tmp/dfs/data/current/BP-193442119-192.168.2.120-1432457733977/current/finalized</code></li></ul></li></ul><h2 id="HDFS应用开发篇"><a href="#HDFS应用开发篇" class="headerlink" title="HDFS应用开发篇"></a>HDFS应用开发篇</h2><h3 id="HDFS的java操作"><a href="#HDFS的java操作" class="headerlink" title="HDFS的java操作"></a>HDFS的java操作</h3><p>hdfs在生产应用中主要是客户端的开发，其核心步骤是从hdfs提供的api中构造一个HDFS的访问客户端对象，然后通过该客户端对象操作（增删改查）HDFS上的文件</p><h3 id="搭建开发环境"><a href="#搭建开发环境" class="headerlink" title="搭建开发环境"></a>搭建开发环境</h3><ol><li>引入依赖</li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.6.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>注：如需手动引入jar包，hdfs的jar包—-hadoop的安装目录的share下</p><ol><li>window下开发的说明<ol><li>建议在linux下进行hadoop应用的开发，不会存在兼容性问题。如在window上做客户端应用开发，需要设置以下环境：<br>A. 在windows的某个目录下解压一个hadoop的安装包<br>B. 将安装包下的lib和bin目录用对应windows版本平台编译的本地库替换<br>C. 在window系统中配置HADOOP_HOME指向你解压的安装包<br>D. 在windows系统的path变量中加入hadoop的bin目录</li></ol></li></ol><h3 id="获取api中的客户端对象"><a href="#获取api中的客户端对象" class="headerlink" title="获取api中的客户端对象"></a>获取api中的客户端对象</h3><ul><li>在java中操作hdfs，首先要获得一个客户端实例</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Configuration conf = <span class="keyword">new</span> Configuration()</span><br><span class="line">FileSystem fs = FileSystem.get(conf)</span><br></pre></td></tr></table></figure><ul><li>而我们的操作目标是HDFS，所以获取到的fs对象应该是DistributedFileSystem的实例；</li><li>get方法是从何处判断具体实例化那种客户端类呢？<ul><li>从conf中的一个参数 fs.defaultFS的配置值判断；</li></ul></li><li>如果我们的代码中没有指定fs.defaultFS，并且工程classpath下也没有给定相应的配置，conf中的默认值就来自于hadoop的jar包中的core-default.xml，默认值为： file:///，则获取的将不是一个DistributedFileSystem的实例，而是一个本地文件系统的客户端对象</li></ul><h3 id="DistributedFileSystem实例对象所具备的方法"><a href="#DistributedFileSystem实例对象所具备的方法" class="headerlink" title="DistributedFileSystem实例对象所具备的方法"></a>DistributedFileSystem实例对象所具备的方法</h3><p><img src="http://ou9crezlk.bkt.clouddn.com/blog/171023/B09g2g05Fi.png?imageslim" alt="mark"></p><h3 id="HDFS客户端操作数据代码示例"><a href="#HDFS客户端操作数据代码示例" class="headerlink" title="HDFS客户端操作数据代码示例"></a>HDFS客户端操作数据代码示例</h3><h4 id="文件的增删改查"><a href="#文件的增删改查" class="headerlink" title="文件的增删改查"></a>文件的增删改查</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.net.URI;</span><br><span class="line"><span class="keyword">import</span> java.util.Iterator;</span><br><span class="line"><span class="keyword">import</span> java.util.Map.Entry;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.BlockLocation;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileStatus;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.FileSystem;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.LocatedFileStatus;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.Path;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.fs.RemoteIterator;</span><br><span class="line"><span class="keyword">import</span> org.junit.Before;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 客户端去操作hdfs时，是有一个用户身份的</span></span><br><span class="line"><span class="comment"> * 默认情况下，hdfs客户端api会从jvm中获取一个参数来作为自己的用户身份：-DHADOOP_USER_NAME=hadoop</span></span><br><span class="line"><span class="comment"> * </span></span><br><span class="line"><span class="comment"> * 也可以在构造客户端fs对象时，通过参数传递进去</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span></span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HdfsClientDemo</span> </span>&#123;</span><br><span class="line">FileSystem fs = <span class="keyword">null</span>;</span><br><span class="line">Configuration conf = <span class="keyword">null</span>;</span><br><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> Exception</span>&#123;</span><br><span class="line"></span><br><span class="line">conf = <span class="keyword">new</span> Configuration();</span><br><span class="line"><span class="comment">//conf.set("fs.defaultFS", "hdfs://mini1:9000");</span></span><br><span class="line">conf.set(<span class="string">"dfs.replication"</span>, <span class="string">"5"</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//拿到一个文件系统操作的客户端实例对象</span></span><br><span class="line">fs = FileSystem.get(conf);</span><br><span class="line"><span class="comment">//可以直接传入 uri和用户身份</span></span><br><span class="line">fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://mini1:9000"</span>),conf,<span class="string">"hadoop"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 上传文件</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testUpload</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">fs.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">"c:/access.log"</span>), <span class="keyword">new</span> Path(<span class="string">"/access.log.copy"</span>));</span><br><span class="line">fs.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 下载文件</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testDownload</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">fs.copyToLocalFile(<span class="keyword">new</span> Path(<span class="string">"/access.log.copy"</span>), <span class="keyword">new</span> Path(<span class="string">"d:/"</span>));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 打印参数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testConf</span><span class="params">()</span></span>&#123;</span><br><span class="line"></span><br><span class="line">Iterator&lt;Entry&lt;String, String&gt;&gt; it = conf.iterator();</span><br><span class="line"><span class="keyword">while</span>(it.hasNext())&#123;</span><br><span class="line">Entry&lt;String, String&gt; ent = it.next();</span><br><span class="line">System.out.println(ent.getKey() + <span class="string">" : "</span> + ent.getValue());</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testMkdir</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="keyword">boolean</span> mkdirs = fs.mkdirs(<span class="keyword">new</span> Path(<span class="string">"/testmkdir/aaa/bbb"</span>));</span><br><span class="line">System.out.println(mkdirs);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testDelete</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">boolean</span> flag = fs.delete(<span class="keyword">new</span> Path(<span class="string">"/testmkdir/aaa"</span>), <span class="keyword">true</span>);</span><br><span class="line">System.out.println(flag);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 递归列出指定目录下所有子文件夹中的文件</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testLs</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">RemoteIterator&lt;LocatedFileStatus&gt; listFiles = fs.listFiles(<span class="keyword">new</span> Path(<span class="string">"/"</span>), <span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(listFiles.hasNext())&#123;</span><br><span class="line">LocatedFileStatus fileStatus = listFiles.next();</span><br><span class="line">System.out.println(<span class="string">"blocksize: "</span> +fileStatus.getBlockSize());</span><br><span class="line">System.out.println(<span class="string">"owner: "</span> +fileStatus.getOwner());</span><br><span class="line">System.out.println(<span class="string">"Replication: "</span> +fileStatus.getReplication());</span><br><span class="line">System.out.println(<span class="string">"Permission: "</span> +fileStatus.getPermission());</span><br><span class="line">System.out.println(<span class="string">"Name: "</span> +fileStatus.getPath().getName());</span><br><span class="line">System.out.println(<span class="string">"------------------"</span>);</span><br><span class="line">BlockLocation[] blockLocations = fileStatus.getBlockLocations();</span><br><span class="line"><span class="keyword">for</span>(BlockLocation b:blockLocations)&#123;</span><br><span class="line">System.out.println(<span class="string">"块起始偏移量: "</span> +b.getOffset());</span><br><span class="line">System.out.println(<span class="string">"块长度:"</span> + b.getLength());</span><br><span class="line"><span class="comment">//块所在的datanode节点</span></span><br><span class="line">String[] datanodes = b.getHosts();</span><br><span class="line"><span class="keyword">for</span>(String dn:datanodes)&#123;</span><br><span class="line">System.out.println(<span class="string">"datanode:"</span> + dn);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testLs2</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">FileStatus[] listStatus = fs.listStatus(<span class="keyword">new</span> Path(<span class="string">"/"</span>));</span><br><span class="line"><span class="keyword">for</span>(FileStatus file :listStatus)&#123;</span><br><span class="line"></span><br><span class="line">System.out.println(<span class="string">"name: "</span> + file.getPath().getName());</span><br><span class="line">System.out.println((file.isFile()?<span class="string">"file"</span>:<span class="string">"directory"</span>));</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">conf.set(<span class="string">"fs.defaultFS"</span>, <span class="string">"hdfs://mini1:9000"</span>);</span><br><span class="line"><span class="comment">//拿到一个文件系统操作的客户端实例对象</span></span><br><span class="line">FileSystem fs = FileSystem.get(conf);</span><br><span class="line"></span><br><span class="line">fs.copyFromLocalFile(<span class="keyword">new</span> Path(<span class="string">"c:/access.log"</span>), <span class="keyword">new</span> Path(<span class="string">"/access.log.copy"</span>));</span><br><span class="line">fs.close();</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="通过流的方式访问hdfs"><a href="#通过流的方式访问hdfs" class="headerlink" title="通过流的方式访问hdfs"></a>通过流的方式访问hdfs</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 相对那些封装好的方法而言的更底层一些的操作方式</span></span><br><span class="line"><span class="comment"> * 上层那些mapreduce   spark等运算框架，去hdfs中获取数据的时候，就是调的这种底层的api</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span></span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StreamAccess</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">FileSystem fs = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">Configuration conf = <span class="keyword">new</span> Configuration();</span><br><span class="line">fs = FileSystem.get(<span class="keyword">new</span> URI(<span class="string">"hdfs://hdp-node01:9000"</span>), conf, <span class="string">"hadoop"</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 通过流的方式上传文件到hdfs</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testUpload</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"></span><br><span class="line">FSDataOutputStream outputStream = fs.create(<span class="keyword">new</span> Path(<span class="string">"/angelababy.love"</span>), <span class="keyword">true</span>);</span><br><span class="line">FileInputStream inputStream = <span class="keyword">new</span> FileInputStream(<span class="string">"c:/angelababy.love"</span>);</span><br><span class="line"></span><br><span class="line">IOUtils.copy(inputStream, outputStream);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testDownLoadFileToLocal</span><span class="params">()</span> <span class="keyword">throws</span> IllegalArgumentException, IOException</span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">//先获取一个文件的输入流----针对hdfs上的</span></span><br><span class="line">FSDataInputStream in = fs.open(<span class="keyword">new</span> Path(<span class="string">"/jdk-7u65-linux-i586.tar.gz"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">//再构造一个文件的输出流----针对本地的</span></span><br><span class="line">FileOutputStream out = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"c:/jdk.tar.gz"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">//再将输入流中数据传输到输出流</span></span><br><span class="line">IOUtils.copyBytes(in, out, <span class="number">4096</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * hdfs支持随机定位进行文件读取，而且可以方便地读取指定长度</span></span><br><span class="line"><span class="comment"> * 用于上层分布式运算框架并发处理数据</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IllegalArgumentException</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testRandomAccess</span><span class="params">()</span> <span class="keyword">throws</span> IllegalArgumentException, IOException</span>&#123;</span><br><span class="line"><span class="comment">//先获取一个文件的输入流----针对hdfs上的</span></span><br><span class="line">FSDataInputStream in = fs.open(<span class="keyword">new</span> Path(<span class="string">"/iloveyou.txt"</span>));</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">//可以将流的起始偏移量进行自定义</span></span><br><span class="line">in.seek(<span class="number">22</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">//再构造一个文件的输出流----针对本地的</span></span><br><span class="line">FileOutputStream out = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"c:/iloveyou.line.2.txt"</span>));</span><br><span class="line"></span><br><span class="line">IOUtils.copyBytes(in,out,<span class="number">19L</span>,<span class="keyword">true</span>);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 显示hdfs上文件的内容</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IOException </span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> IllegalArgumentException </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCat</span><span class="params">()</span> <span class="keyword">throws</span> IllegalArgumentException, IOException</span>&#123;</span><br><span class="line"></span><br><span class="line">FSDataInputStream in = fs.open(<span class="keyword">new</span> Path(<span class="string">"/iloveyou.txt"</span>));</span><br><span class="line"></span><br><span class="line">IOUtils.copyBytes(in, System.out, <span class="number">1024</span>);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="场景编程"><a href="#场景编程" class="headerlink" title="场景编程"></a>场景编程</h4><p>在mapreduce 、spark等运算框架中，有一个核心思想就是将运算移往数据，或者说，就是要在并发计算中尽可能让运算本地化，这就需要获取数据所在位置的信息并进行相应范围读取以下模拟实现：获取一个文件的所有block位置信息，然后读取指定block中的内容</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">testCat</span><span class="params">()</span> <span class="keyword">throws</span> IllegalArgumentException, IOException</span>&#123;</span><br><span class="line"></span><br><span class="line">FSDataInputStream in = fs.open(<span class="keyword">new</span> Path(<span class="string">"/wordcount/data"</span>));</span><br><span class="line"><span class="comment">//拿到文件信息</span></span><br><span class="line">FileStatus[] listStatus = fs.listStatus(<span class="keyword">new</span> Path(<span class="string">"/wordcount/data"</span>));</span><br><span class="line"><span class="comment">//获取这个文件的所有block的信息</span></span><br><span class="line">BlockLocation[] fileBlockLocations = fs.getFileBlockLocations(listStatus[<span class="number">0</span>], <span class="number">0L</span>, listStatus[<span class="number">0</span>].getLen());</span><br><span class="line"><span class="comment">//第一个block的长度</span></span><br><span class="line"><span class="keyword">long</span> length = fileBlockLocations[<span class="number">0</span>].getLength();</span><br><span class="line"><span class="comment">//第一个block的起始偏移量</span></span><br><span class="line"><span class="keyword">long</span> offset = fileBlockLocations[<span class="number">0</span>].getOffset();</span><br><span class="line"></span><br><span class="line">System.out.println(length);</span><br><span class="line">System.out.println(offset);</span><br><span class="line"></span><br><span class="line"><span class="comment">//获取第一个block写入输出流</span></span><br><span class="line"><span class="comment">//IOUtils.copyBytes(in, System.out, (int)length);</span></span><br><span class="line"><span class="keyword">byte</span>[] b = <span class="keyword">new</span> <span class="keyword">byte</span>[<span class="number">4096</span>];</span><br><span class="line"></span><br><span class="line">FileOutputStream os = <span class="keyword">new</span> FileOutputStream(<span class="keyword">new</span> File(<span class="string">"d:/block0"</span>));</span><br><span class="line"><span class="keyword">while</span>(in.read(offset, b, <span class="number">0</span>, <span class="number">4096</span>)!=-<span class="number">1</span>)&#123;</span><br><span class="line">os.write(b);</span><br><span class="line">offset += <span class="number">4096</span>;</span><br><span class="line"><span class="keyword">if</span>(offset&gt;=length) <span class="keyword">return</span>;</span><br><span class="line">&#125;;</span><br><span class="line">os.flush();</span><br><span class="line">os.close();</span><br><span class="line">in.close();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;HDFS前言&quot;&gt;&lt;a href=&quot;#HDFS前言&quot; class=&quot;headerlink&quot; title=&quot;HDFS前言&quot;&gt;&lt;/a&gt;HDFS前言&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;设计思想&lt;ul&gt;
&lt;li&gt;分而治之：将大文件、大批量文件，分布式存放在大量服务器上，&lt;stron
      
    
    </summary>
    
      <category term="Java" scheme="http://yoursite.com/categories/Java/"/>
    
    
      <category term="Hadoop" scheme="http://yoursite.com/tags/Hadoop/"/>
    
      <category term="Java" scheme="http://yoursite.com/tags/Java/"/>
    
      <category term="HDFS" scheme="http://yoursite.com/tags/HDFS/"/>
    
  </entry>
  
  <entry>
    <title>Linux VIM文本编辑器</title>
    <link href="http://yoursite.com/2018/03/06/Linux%20VIM%E6%96%87%E6%9C%AC%E7%BC%96%E8%BE%91%E5%99%A8/"/>
    <id>http://yoursite.com/2018/03/06/Linux VIM文本编辑器/</id>
    <published>2018-03-06T03:50:35.000Z</published>
    <updated>2018-03-06T06:59:08.133Z</updated>
    
    <content type="html"><![CDATA[<p>##VIM文本编辑器<br><img src="http://ou9crezlk.bkt.clouddn.com/blog/170811/1BLkCHBIj5.png" alt="mark"></p><p>###插入命令<br><img src="http://ou9crezlk.bkt.clouddn.com/blog/170811/8bjIDm0B76.png" alt="mark"></p><p>###定位命令<br><img src="http://ou9crezlk.bkt.clouddn.com/blog/170811/J65aj3C260.png" alt="mark"></p><p>###替换和取消命令<br><img src="http://ou9crezlk.bkt.clouddn.com/blog/170811/aeFBblBdHA.png" alt="mark"></p><p>###删除命令<br><img src="http://ou9crezlk.bkt.clouddn.com/blog/170811/92iAcHk0LJ.png" alt="mark"></p><p>###常用快捷键<br><img src="http://ou9crezlk.bkt.clouddn.com/blog/170811/38L1j4dbmF.png" alt="mark"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;##VIM文本编辑器&lt;br&gt;&lt;img src=&quot;http://ou9crezlk.bkt.clouddn.com/blog/170811/1BLkCHBIj5.png&quot; alt=&quot;mark&quot;&gt;&lt;/p&gt;
&lt;p&gt;###插入命令&lt;br&gt;&lt;img src=&quot;http://ou9cr
      
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux YUM命令</title>
    <link href="http://yoursite.com/2018/03/06/Linux%20YUM%E5%91%BD%E4%BB%A4/"/>
    <id>http://yoursite.com/2018/03/06/Linux YUM命令/</id>
    <published>2018-03-06T03:50:35.000Z</published>
    <updated>2018-03-06T06:59:27.047Z</updated>
    
    <content type="html"><![CDATA[<p>##YUM命令</p><ul><li>Yum（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及SUSE、CentOS中的Shell前端软件包管理器。基於RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以自动处理依赖性关系，并且一次安装所有依赖的软件包，无须繁琐地一次次下载、安装。</li><li>例子（需要上网，没有网络可以建本地源）：<blockquote><p>yum  install  gcc-c++<br>m  remove  gcc-c++<br>m  update  gcc-c++</p></blockquote></li></ul><p>###常用命令</p><ul><li>安装httpd并确认安装<ul><li><code>yum instll -y httpd</code></li></ul></li><li>列出所有可用的package和package组<ul><li><code>yum list</code></li></ul></li><li>清除所有缓冲数据<ul><li><code>yum clean all</code></li></ul></li><li>列出一个包所有依赖的包<ul><li><code>yum deplist httpd</code></li></ul></li><li>删除httpd<ul><li><code>yum remove httpd</code></li></ul></li></ul><p>##制作YUM本地源</p><ul><li><p>YUM源其实就是一个保存了多个RPM包的服务器，可以通过http的方式来检索、下载并安装相关的RPM包</p></li><li><p>制作本地YUM源</p></li></ul><ol><li>准备一台Linux服务器，用最简单的版本CentOS-6.7-x86_64-minimal.iso</li><li>配置好这台服务器的IP地址</li><li>上传CentOS-6.7-x86_64-bin-DVD1.iso到服务器</li><li><p>将CentOS-6.7-x86_64-bin-DVD1.iso镜像挂载到某个目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /var/iso</span><br><span class="line">mount -o loop CentOS-6.7-x86_64-bin-DVD1.iso /var/iso</span><br></pre></td></tr></table></figure></li><li><p>修改本机上的YUM源配置文件，将源指向自己<br>备份原有的YUM源的配置文件</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/yum.repos.d/</span><br><span class="line">rename .repo .repo.bak *</span><br><span class="line">vi CentOS-Local.repo</span><br></pre></td></tr></table></figure><blockquote><p>[base]<br>name=CentOS-Local<br>baseurl=file:///var/iso<br>gpgcheck=1<br> nabled=1   #很重要，1才启用<br>gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6</p></blockquote><p>添加上面内容保存退出</p><ol><li>清除YUM缓冲<br>yum clean all</li><li>列出可用的YUM源<br>yum repolist</li><li>安装相应的软件<br>yum install -y httpd</li><li>开启httpd使用浏览器访问<a href="http://192.168.0.100:80（如果访问不通，检查防火墙是否开启了80端口或关闭防火墙）" target="_blank" rel="noopener">http://192.168.0.100:80（如果访问不通，检查防火墙是否开启了80端口或关闭防火墙）</a><br>service httpd start</li><li>将YUM源配置到httpd（Apache Server）中，其他的服务器即可通过网络访问这个内网中的YUM源了<br>cp -r /var/iso/ /var/www/html/CentOS-6.7</li><li>取消先前挂载的镜像<br>umount /var/iso</li><li>在浏览器中访问<a href="http://192.168.0.100/CentOS-6.7/" target="_blank" rel="noopener">http://192.168.0.100/CentOS-6.7/</a><br><img src="http://ou9crezlk.bkt.clouddn.com/blog/170831/7d1C4lf4l4.png" alt="mark"><br>13.让其他需要安装RPM包的服务器指向这个YUM源，准备一台新的服务器，备份或删除原有的YUM源配置文件<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd /etc/yum.repos.d/</span><br><span class="line">rename .repo .repo.bak *</span><br><span class="line">vi CentOS-Local.repo</span><br></pre></td></tr></table></figure></li></ol><blockquote><p>[base]<br>name=CentOS-Local<br>baseurl=<a href="http://192.168.0.100/CentOS-6.7" target="_blank" rel="noopener">http://192.168.0.100/CentOS-6.7</a><br>gpgcheck=1<br>gpgkey=file:///etc/pki/rpm-gpg/RPM-GPG-KEY-CentOS-6</p></blockquote><p>添加上面内容保存退出<br>14.在这台新的服务器上执行YUM的命令<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum clean all</span><br><span class="line">yum repolist</span><br></pre></td></tr></table></figure></p><p>15.安装相应的软件<br><code>yum install -y gcc</code></p><p>16、加入依赖包到私有yum的repository<br>进入到repo目录<br>执行命令：  <code>createrepo  .</code></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;##YUM命令&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Yum（全称为 Yellow dog Updater, Modified）是一个在Fedora和RedHat以及SUSE、CentOS中的Shell前端软件包管理器。基於RPM包管理，能够从指定的服务器自动下载RPM包并且安装，可以
      
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux cut 提取文本列&amp;&amp;wc 统计命令</title>
    <link href="http://yoursite.com/2018/03/06/Linux%20cut%20%E6%8F%90%E5%8F%96%E6%96%87%E6%9C%AC%E5%88%97&amp;&amp;wc%20%E7%BB%9F%E8%AE%A1%E5%91%BD%E4%BB%A4/"/>
    <id>http://yoursite.com/2018/03/06/Linux cut 提取文本列&amp;&amp;wc 统计命令/</id>
    <published>2018-03-06T03:50:35.000Z</published>
    <updated>2018-03-06T06:58:19.518Z</updated>
    
    <content type="html"><![CDATA[<p>##cut命令</p><ul><li>cut命令可以从一个文本文件或者文本流中提取文本列。</li></ul><p>###cut语法</p><ul><li><code>cut -d&#39;分隔字符&#39; -f fields &lt;==用于有特定分隔字符</code></li><li><code>cut -c 字符区间            &lt;==用于排列整齐的信息</code></li><li>选项与参数：<ul><li>-d  ：后面接分隔字符。与 -f 一起使用；</li><li>-f  ：依据 -d 的分隔字符将一段信息分割成为数段，用 -f 取出第几段的意思；</li><li>-c  ：以字符 (characters) 的单位取出固定字符区间；</li></ul></li></ul><p>###举个栗子</p><ul><li><p>PATH 变量如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">echo $PATH</span><br><span class="line">/bin:/usr/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/X11R6/bin:/usr/games</span><br><span class="line"> 1 | 2       | 3   | 4       | 5            | 6            | 7</span><br></pre></td></tr></table></figure></li><li><p>语句：<code>echo $PATH | cut -d &#39;:&#39; -f 5</code></p></li><li><p>结果：将 PATH 变量取出，找出第五个路径。</p><blockquote><p>/usr/local/bin</p></blockquote></li><li><p>语句：<code>echo $PATH | cut -d &#39;:&#39; -f 3,5</code></p></li><li><p>结果：将 PATH 变量取出，我要找出第三和第五个路径。</p><blockquote><p>/sbin:/usr/local/bin</p></blockquote></li><li><p>语句：<code>echo $PATH | cut -d &#39;:&#39; -f 3-</code></p></li><li><p>结果：将 PATH 变量取出，我要找出第三到最后一个路径。</p><blockquote><p>/sbin:/usr/sbin:/usr/local/bin:/usr/X11R6/bin:/usr/games</p></blockquote></li><li><p>语句：<code>echo $PATH | cut -d &#39;:&#39; -f 1-3</code></p></li><li>结果：将 PATH 变量取出，我要找出第一到第三个路径。<blockquote><p>/bin:/usr/bin:/sbin:</p></blockquote></li></ul><ul><li>语句：<code>echo $PATH | cut -d &#39;:&#39; -f 1-3,5</code></li><li>结果：将 PATH 变量取出，我要找出第一到第三，还有第五个路径。<blockquote><p>/bin:/usr/bin:/sbin:/usr/local/bin</p></blockquote></li></ul><p>###实用例子</p><ul><li>语句：<code>cat /etc/passwd | cut -d &#39;:&#39; -f 1,7</code></li><li>结果：只显示/etc/passwd的用户和shell<blockquote><p>root:/bin/bash<br>daemon:/bin/sh<br>bin:/bin/sh</p></blockquote></li></ul><p>##wc命令</p><ul><li>统计文件里面有多少单词，多少行，多少字符。</li></ul><p>###wc语法</p><ul><li><code>wc [-lwm]</code></li><li>选项与参数：<ul><li>-l  ：仅列出行；</li><li>-w  ：仅列出多少字(英文单字)；</li><li>-m  ：多少字符；</li></ul></li></ul><p>###举个栗子</p><ul><li><p>使用wc统计/etc/passwd</p></li><li><p>语句：<code>wc /etc/passwd</code></p></li><li><p>结果：40是行数，45是单词数，1719是字节数</p><blockquote><p>40   45 1719 /etc/passwd</p></blockquote></li><li><p>语句：<code>wc -l /etc/passwd</code></p></li><li><p>结果：统计行数,表示系统有40个账户</p><blockquote><p>40 /etc/passwd</p></blockquote></li><li><p>语句：<code>wc -w /etc/passwd</code></p></li><li><p>结果：统计单词出现次数</p><blockquote><p>45 /etc/passwd</p></blockquote></li><li><p>语句：<code>wc -m /etc/passwd</code></p></li><li>结果：统计文件的字符数<blockquote><p>1719</p></blockquote></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;##cut命令&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;cut命令可以从一个文本文件或者文本流中提取文本列。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;###cut语法&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;cut -d&amp;#39;分隔字符&amp;#39; -f fields &amp;lt;==用于有特定分隔字符&lt;
      
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Let&#39;s Encrypt 使用教程，免费的 SSL 证书</title>
    <link href="http://yoursite.com/2018/03/06/Let&#39;s%20Encrypt%20%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%EF%BC%8C%E5%85%8D%E8%B4%B9%E7%9A%84%20SSL%20%E8%AF%81%E4%B9%A6/"/>
    <id>http://yoursite.com/2018/03/06/Let&#39;s Encrypt 使用教程，免费的 SSL 证书/</id>
    <published>2018-03-06T03:50:35.000Z</published>
    <updated>2018-03-06T06:57:36.585Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Let’s-Encrypt-简介"><a href="#Let’s-Encrypt-简介" class="headerlink" title="Let’s Encrypt 简介"></a>Let’s Encrypt 简介</h2><p>Let’s Encrypt 是国外一个公共的免费SSL项目，由 Linux 基金会托管，它的来头不小，由 Mozilla、思科、Akamai、IdenTrust 和 EFF 等组织发起，目的就是向网站自动签发和管理免费证书，以便加速互联网由 HTTP 过渡到 HTTPS，目前 Facebook 等大公司开始加入赞助行列。</p><p>Let’s Encrypt 已经得了 IdenTrust 的交叉签名，这意味着其证书现在已经可以被 Mozilla、Google、Microsoft 和 Apple 等主流的浏览器所信任，你只需要在 Web 服务器证书链中配置交叉签名，浏览器客户端会自动处理好其它的一切，Let’s Encrypt 安装简单，使用非常方便。</p><h2 id="Certbot-简介"><a href="#Certbot-简介" class="headerlink" title="Certbot 简介"></a>Certbot 简介</h2><p>Certbot 为 Let’s Encrypt 项目发布了一个官方的客户端 Certbot ，利用它可以完全自动化的获取、部署和更新安全证书，并且 Certbot 是支持所有 Unix 内核的操作系统。</p><h2 id="安装-Certbot-客户端"><a href="#安装-Certbot-客户端" class="headerlink" title="安装 Certbot 客户端"></a>安装 Certbot 客户端</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ yum install certbot # centos</span><br><span class="line">$ # apt install certbot # ubuntu</span><br></pre></td></tr></table></figure><h3 id="Certbot-的两种使用方式"><a href="#Certbot-的两种使用方式" class="headerlink" title="Certbot 的两种使用方式"></a>Certbot 的两种使用方式</h3><p>webroot 方式： certbot 会利用既有的 web server，在其 web root 目录下创建隐藏文件，Let’s Encrypt 服务端会通过域名来访问这些隐藏文件，以确认你的确拥有对应域名的控制权。<br>standalone 方式： Certbot 会自己运行一个 web server 来进行验证。如果我们自己的服务器上已经有 web server 正在运行 （比如 Nginx 或 Apache ），用 standalone 方式的话需要先关掉它，以免冲突。<br>获取证书 webroot 模式<br>使用这种模式会在 web root 中创建 .well-known 文件夹，这个文件夹里面包含了一些验证文件，Certbot 会通过访问 example.com/.well-known/acme-challenge 来验证你的域名是否绑定的这个服务器，所以需要编辑 nginx 配置文件确保可以访问刚刚创建的 .well-known 文件夹及里边存放的验证文件，以便在生成证书时进行验证：</p><p>使用一下命令查看 nginx 配置文件地址：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ sudo nginx -t</span><br><span class="line">nginx: the configuration file /usr/local/nginx/nginx.conf syntax is ok  </span><br><span class="line">nginx: configuration file /usr/local/nginx/nginx.conf test is successful</span><br></pre></td></tr></table></figure><p>编辑 /usr/local/nginx/nginx.conf 配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">server &#123;  </span><br><span class="line">    ...</span><br><span class="line">    location /.well-known/acme-challenge/ &#123;</span><br><span class="line">        default_type &quot;text/plain&quot;;</span><br><span class="line">        root /var/www/example;</span><br><span class="line">    &#125;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>重启 nginx 服务</p><p><code>$ nginx -s reload</code>获取证书，–email 为申请者邮箱，–webroot 为 webroot 方式，-w 为站点目录，-d 为要加 https 的域名，以下命令会为 example.com 和 <a href="http://www.example.com" target="_blank" rel="noopener">www.example.com</a> 这两个域名生成一个证书：</p><p><code>$ certbot certonly --email admin@example.com --webroot -w /var/www/example -d example.com -d www.example.com</code>standalone 模式获取证书 但是有些时候我们的一些服务并没有根目录，例如一些微服务，这时候使用 webroot 模式就走不通了。这时可以使用模式 standalone 模式，这种模式不需要指定网站根目录，他会自动启用服务器的443端口，来验证域名的归属。我们有其他服务（例如nginx）占用了443端口，就必须先停止这些服务，在证书生成完毕后，再启用。</p><p><code>$ certbot certonly --email admin@example.com --standalone -d example.com -d www.example.com</code>nginx 开启 https 证书生成完成后可以到 /etc/letsencrypt/live/ 目录下查看对应域名的证书文件。编辑 nginx 配置文件监听 443 端口，启用 SSL，并配置 SSL 的公钥、私钥证书路径：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">server &#123;  </span><br><span class="line">    listen       443;</span><br><span class="line">    server_name  example.com;</span><br><span class="line">    root         /var/www/example;</span><br><span class="line">    index        index.html index.htm;</span><br><span class="line">    ssl on;</span><br><span class="line">    ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem;</span><br><span class="line">    ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem;</span><br><span class="line">    charset utf-8;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>添加 HTTP 跳转到 HTTPS：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">server &#123;  </span><br><span class="line">    listen 80;</span><br><span class="line">    server_name example.com;</span><br><span class="line">    return 301 https://$server_name$request_uri;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>重启 nginx 服务，访问 <a href="https://example.com/" target="_blank" rel="noopener">https://example.com</a> 查看是否配置成功</p><p>自动续期 Let’s Encrypt 提供的证书只有90天的有效期，所以我们要在在证书到期之前重新获取这些证书，Certbot 提供了一个方便的命令 certbot renew，我们可以先使用 –dry-run 测试是否可用：</p><p><code>$ certbot renew --dry-run</code>linux 系统上有 cron 可以来搞定这件事情，使用一下命令新建任务：</p><p><code>$ crontab -e</code>写入一下任务内容。这段内容的意思就是 每隔 两个月的 凌晨 2:15 执行 更新操作</p><p><code>15 2 * */2 * certbot renew --quiet --renew-hook &quot;service nginx restart&quot;</code>参数 表述 –quiet 执行时屏蔽错误以外的所有输出，也可以使用 -q –pre-hook 执行更新操作之前要做的事情 –pre-hook 执行更新操作之前要做的事情 –post-hook 执行更新操作完成后要做的事情 取消证书 可以使用一下命令取消刚刚生成的密匙，也就是以上的反操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ certbot revoke --cert-path /etc/letsencrypt/live/example.com/cert.pem</span><br><span class="line">$ certbot delete --cert-name example.com</span><br></pre></td></tr></table></figure><p>至此，整个网站升级到 HTTPS 就完成了。 七牛静态资源也能使用 HTTPS 如果博客里面没有用到外部的一些静态图片等资源，那这时候访问网站，应该会看到地址栏有一把小绿锁了。但是我的博客一直使用的是七牛的免费图床，一直都是 HTTP 外链，所以地址栏的小绿锁也没有显示。</p><p>结束语 如果加上 HTTPS 之后😊锁不够绿的话，检查下站点加载的资源（比如 js、css、照片等）是不是有 HTTP 的，有的话就会导致小锁变为灰色。 在我们使用 HTTP 的时候，打开网页总会遇到第三方偷偷加的一些脚本广告，很是烦人，升级 HTTPS 后他们就无从下手了，欧耶。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;Let’s-Encrypt-简介&quot;&gt;&lt;a href=&quot;#Let’s-Encrypt-简介&quot; class=&quot;headerlink&quot; title=&quot;Let’s Encrypt 简介&quot;&gt;&lt;/a&gt;Let’s Encrypt 简介&lt;/h2&gt;&lt;p&gt;Let’s Encrypt 
      
    
    </summary>
    
      <category term="Https" scheme="http://yoursite.com/categories/Https/"/>
    
    
      <category term="Https" scheme="http://yoursite.com/tags/Https/"/>
    
      <category term="Let&#39;s Encrypt" scheme="http://yoursite.com/tags/Let-s-Encrypt/"/>
    
  </entry>
  
  <entry>
    <title>Java反射</title>
    <link href="http://yoursite.com/2018/03/06/Java%E5%8F%8D%E5%B0%84/"/>
    <id>http://yoursite.com/2018/03/06/Java反射/</id>
    <published>2018-03-06T03:50:35.000Z</published>
    <updated>2018-03-06T06:54:53.679Z</updated>
    
    <content type="html"><![CDATA[<p>##反射的代码示例</p><ul><li>通过反射的方式可以获取class对象中的属性、方法、构造函数等，一下是实例</li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.java.reflect;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Constructor;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Field;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Method;</span><br><span class="line"><span class="keyword">import</span> java.util.ArrayList;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.junit.Before;</span><br><span class="line"><span class="keyword">import</span> org.junit.Test;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyReflect</span> </span>&#123;</span><br><span class="line"><span class="keyword">public</span> String className = <span class="keyword">null</span>;</span><br><span class="line"><span class="meta">@SuppressWarnings</span>(<span class="string">"rawtypes"</span>)</span><br><span class="line"><span class="keyword">public</span> Class personClass = <span class="keyword">null</span>;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 反射Person类</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@throws</span> Exception </span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Before</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">init</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">className = <span class="string">"cn.java.reflect.Person"</span>;</span><br><span class="line">personClass = Class.forName(className);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *获取某个class文件对象</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getClassName</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">System.out.println(personClass);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *获取某个class文件对象的另一种方式</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getClassName2</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">System.out.println(Person.class);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *创建一个class文件表示的真实对象，底层会调用空参数的构造方法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getNewInstance</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">System.out.println(personClass.newInstance());</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *获取非私有的构造函数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@SuppressWarnings</span>(&#123; <span class="string">"rawtypes"</span>, <span class="string">"unchecked"</span> &#125;)</span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getPublicConstructor</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Constructor  constructor  = personClass.getConstructor(Long.class,String.class);</span><br><span class="line">Person person = (Person)constructor.newInstance(<span class="number">100L</span>,<span class="string">"zhangsan"</span>);</span><br><span class="line">System.out.println(person.getId());</span><br><span class="line">System.out.println(person.getName());</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *获得私有的构造函数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@SuppressWarnings</span>(&#123; <span class="string">"rawtypes"</span>, <span class="string">"unchecked"</span> &#125;)</span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getPrivateConstructor</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Constructor con = personClass.getDeclaredConstructor(String.class);</span><br><span class="line">con.setAccessible(<span class="keyword">true</span>);<span class="comment">//强制取消Java的权限检测</span></span><br><span class="line">Person person2 = (Person)con.newInstance(<span class="string">"zhangsan"</span>);</span><br><span class="line">System.out.println(person2.getName());</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *获取非私有的成员变量</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@SuppressWarnings</span>(&#123; <span class="string">"rawtypes"</span>, <span class="string">"unchecked"</span> &#125;)</span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getNotPrivateField</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Constructor  constructor  = personClass.getConstructor(Long.class,String.class);</span><br><span class="line">Object obj = constructor.newInstance(<span class="number">100L</span>,<span class="string">"zhangsan"</span>);</span><br><span class="line"></span><br><span class="line">Field field = personClass.getField(<span class="string">"name"</span>);</span><br><span class="line">field.set(obj, <span class="string">"lisi"</span>);</span><br><span class="line">System.out.println(field.get(obj));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *获取私有的成员变量</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@SuppressWarnings</span>(&#123; <span class="string">"rawtypes"</span>, <span class="string">"unchecked"</span> &#125;)</span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getPrivateField</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Constructor  constructor  = personClass.getConstructor(Long.class);</span><br><span class="line">Object obj = constructor.newInstance(<span class="number">100L</span>);</span><br><span class="line"></span><br><span class="line">Field field2 = personClass.getDeclaredField(<span class="string">"id"</span>);</span><br><span class="line">field2.setAccessible(<span class="keyword">true</span>);<span class="comment">//强制取消Java的权限检测</span></span><br><span class="line">field2.set(obj,<span class="number">10000L</span>);</span><br><span class="line">System.out.println(field2.get(obj));</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *获取非私有的成员函数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@SuppressWarnings</span>(&#123; <span class="string">"unchecked"</span> &#125;)</span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getNotPrivateMethod</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">System.out.println(personClass.getMethod(<span class="string">"toString"</span>));</span><br><span class="line"></span><br><span class="line">Object obj = personClass.newInstance();<span class="comment">//获取空参的构造函数</span></span><br><span class="line">Object object = personClass.getMethod(<span class="string">"toString"</span>).invoke(obj);</span><br><span class="line">System.out.println(object);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *获取私有的成员函数</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@SuppressWarnings</span>(<span class="string">"unchecked"</span>)</span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getPrivateMethod</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">Object obj = personClass.newInstance();<span class="comment">//获取空参的构造函数</span></span><br><span class="line">Method method = personClass.getDeclaredMethod(<span class="string">"getSomeThing"</span>);</span><br><span class="line">method.setAccessible(<span class="keyword">true</span>);</span><br><span class="line">Object value = method.invoke(obj);</span><br><span class="line">System.out.println(value);</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="meta">@Test</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">otherMethod</span><span class="params">()</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line"><span class="comment">//当前加载这个class文件的那个类加载器对象</span></span><br><span class="line">System.out.println(personClass.getClassLoader());</span><br><span class="line"><span class="comment">//获取某个类实现的所有接口</span></span><br><span class="line">Class[] interfaces = personClass.getInterfaces();</span><br><span class="line"><span class="keyword">for</span> (Class class1 : interfaces) &#123;</span><br><span class="line">System.out.println(class1);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">//反射当前这个类的直接父类</span></span><br><span class="line">System.out.println(personClass.getGenericSuperclass());</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * getResourceAsStream这个方法可以获取到一个输入流，这个输入流会关联到name所表示的那个文件上。</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="comment">//path 不以’/'开头时默认是从此类所在的包下取资源，以’/'开头则是从ClassPath根下获取。其只是通过path构造一个绝对路径，最终还是由ClassLoader获取资源。</span></span><br><span class="line">System.out.println(personClass.getResourceAsStream(<span class="string">"/log4j.properties"</span>));</span><br><span class="line"><span class="comment">//默认则是从ClassPath根下获取，path不能以’/'开头，最终是由ClassLoader获取资源。</span></span><br><span class="line">System.out.println(personClass.getResourceAsStream(<span class="string">"/log4j.properties"</span>));</span><br><span class="line"></span><br><span class="line"><span class="comment">//判断当前的Class对象表示是否是数组</span></span><br><span class="line">System.out.println(personClass.isArray());</span><br><span class="line">System.out.println(<span class="keyword">new</span> String[<span class="number">3</span>].getClass().isArray());</span><br><span class="line"></span><br><span class="line"><span class="comment">//判断当前的Class对象表示是否是枚举类</span></span><br><span class="line">System.out.println(personClass.isEnum());</span><br><span class="line">System.out.println(Class.forName(<span class="string">"cn.java.reflect.City"</span>).isEnum());</span><br><span class="line"></span><br><span class="line"><span class="comment">//判断当前的Class对象表示是否是接口</span></span><br><span class="line">System.out.println(personClass.isInterface());</span><br><span class="line">System.out.println(Class.forName(<span class="string">"cn.java.reflect.TestInterface"</span>).isInterface());</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;##反射的代码示例&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;通过反射的方式可以获取class对象中的属性、方法、构造函数等，一下是实例&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&quot;highlight java&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre
      
    
    </summary>
    
      <category term="Java" scheme="http://yoursite.com/categories/Java/"/>
    
    
      <category term="Java" scheme="http://yoursite.com/tags/Java/"/>
    
  </entry>
  
  <entry>
    <title>Linux mini安装常见问题</title>
    <link href="http://yoursite.com/2018/03/06/Linux%20mini%E5%AE%89%E8%A3%85%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
    <id>http://yoursite.com/2018/03/06/Linux mini安装常见问题/</id>
    <published>2018-03-06T03:50:35.000Z</published>
    <updated>2018-03-06T06:58:36.403Z</updated>
    
    <content type="html"><![CDATA[<p>##yum安装网络不可达</p><p>###原因</p><ul><li><p>我们来查看一下，当前用来上网的网卡eth1的信息：</p><blockquote><p>[root@promote ~]# ifconfig eth1<br>th1      Link encap:Ethernet  HWaddr 00:0C:29:D4:EA:E2  </p><pre><code>inet addr:192.168.1.115  Bcast:255.255.255.255  Mask:255.255.255.0inet6 addr: fe80::20c:29ff:fed4:eae2/64 Scope:LinkUP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1RX packets:4599 errors:0 dropped:0 overruns:0 frame:0TX packets:2628 errors:0 dropped:0 overruns:0 carrier:0collisions:0 txqueuelen:1000 RX bytes:575396 (561.9 KiB)  TX bytes:278925 (272.3 KiB)Interrupt:19 Base address:0x2000 </code></pre></blockquote></li><li><p>我们清楚地看见了inet6 addr: fe80::20c:29ff:fed4:eae2/64 Scope:Link,这一行信息，这个正是影响我们使用yum的罪魁祸首！！！下面我们就来关闭网卡的IPV6。</p></li></ul><p>###临时解决方法</p><ul><li>临时关闭网卡的IPV6，并重启网络服务：</li></ul><blockquote><p>[root@promote ~]# echo 1 &gt; /proc/sys/net/ipv6/conf/all/disable_ipv6<br> root@promote ~]# echo 1 &gt; /proc/sys/net/ipv6/conf/default/disable_ipv6<br> root@promote ~]# service network restart<br> 在关闭接口 eth1：                                        [确定]<br> 闭环回接口：                                             [确定]<br> 出环回接口：                                             [确定]<br> 出界面 eth1：<br> 在决定 eth1 的 IP 信息…完成。<br>                                                           [确定]</p></blockquote><ul><li>此时，再查看网卡<code>eth1</code>的信息：</li></ul><blockquote><p>[root@promote ~]# ifconfig eth1<br> th1      Link encap:Ethernet  HWaddr 00:0C:29:D4:EA:E2<br>          inet addr:192.168.1.115  Bcast:255.255.255.255  Mask:255.255.255.0<br>          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1<br>          RX packets:5563 errors:0 dropped:0 overruns:0 frame:0<br>          TX packets:3388 errors:0 dropped:0 overruns:0 carrier:0<br>          collisions:0 txqueuelen:1000<br>          RX bytes:676945 (661.0 KiB)  TX bytes:414358 (404.6 KiB)<br>          Interrupt:19 Base address:0x2000 </p></blockquote><ul><li>没错，我们发现inet6 addr: fe80::20c:29ff:fed4:eae2/64 Scope:Link这一行信息没有了，此时yum就可以正常使用了；</li></ul><p>###永久生效方法</p><ul><li>需要在文件/etc/sysctl.conf中，添加如下的内容：</li></ul><blockquote><p>#shutdown IPv6<br> et.ipv6.conf.all.disable_ipv6 = 1<br> et.ipv6.conf.default.disable_ipv6 = 1</p></blockquote><p>##minimal最小化安装 eth0默认没有自启用</p><ul><li>修改配置文件<ul><li><code>vi /etc/sysconfig/network-scripts/ifcfg-eth0</code><blockquote><p>onboot=true</p></blockquote></li></ul></li></ul><p>##修改静态地址后发现无法ping外网</p><ul><li>需要设置网关（该命令只是临时有效）<ul><li><code>route add default gw 192.168.33.1</code>(网关地址 参考vmware的网关地址)</li></ul></li><li>添加nameserver<ul><li>vi /etc/resolv.conf<blockquote><p>nameserver 192.168.33.1 #vmware的网关地址</p></blockquote></li></ul></li></ul><p>##解决克隆后eth0不见的问题</p><ul><li>直接修改  /etc/sysconfig/network-script/ifcfg-eth0</li><li>删掉UUID  HWADDR</li><li>配置静态地址</li><li>然后：<ul><li><code>rm -rf 　/etc/udev/rules.d/70-persistent-net.rules</code></li></ul></li><li>然后 reboot</li></ul><p>##挂载光驱</p><ul><li>创建需要挂载的目录<ul><li><code>mkdir /mnt/cdrom</code></li></ul></li><li>挂载光驱<ul><li><code>mount -t iso9660 -o ro /dev/cdrom /mnt/cdrom/</code></li></ul></li><li>配置开机自动挂载    <ul><li><code>vim /etc/fstab</code></li><li><code>/dev/cdrom              /mnt/cdrom              iso9660 defaults        0 0</code></li><li><img src="http://ou9crezlk.bkt.clouddn.com/blog/170906/56iEagm84E.png" alt="mark"></li></ul></li></ul><p>##安装scp</p><ul><li><code>yum install -y openssh-clients</code></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;##yum安装网络不可达&lt;/p&gt;
&lt;p&gt;###原因&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;我们来查看一下，当前用来上网的网卡eth1的信息：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[root@promote ~]# ifconfig eth1&lt;br&gt;th1      Link 
      
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
  </entry>
  
  <entry>
    <title>Linux 配置主机之间的免密ssh登陆</title>
    <link href="http://yoursite.com/2018/03/06/Linux%20%E9%85%8D%E7%BD%AE%E4%B8%BB%E6%9C%BA%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%8D%E5%AF%86ssh%E7%99%BB%E9%99%86/"/>
    <id>http://yoursite.com/2018/03/06/Linux 配置主机之间的免密ssh登陆/</id>
    <published>2018-03-06T03:50:35.000Z</published>
    <updated>2018-03-06T07:00:05.855Z</updated>
    
    <content type="html"><![CDATA[<p>##配置主机之间的免密ssh登陆</p><blockquote><p>假如 A  要登陆  B<br>在A上操作：<br>%%首先生成密钥对<br>  h-keygen   (提示时，直接回车即可)<br>%%再将A自己的公钥拷贝并追加到B的授权列表文件authorized_keys中<br>  h-copy-id   B</p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;##配置主机之间的免密ssh登陆&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;假如 A  要登陆  B&lt;br&gt;在A上操作：&lt;br&gt;%%首先生成密钥对&lt;br&gt;  h-keygen   (提示时，直接回车即可)&lt;br&gt;%%再将A自己的公钥拷贝并追加到B的授权列表文件authoriz
      
    
    </summary>
    
      <category term="Linux" scheme="http://yoursite.com/categories/Linux/"/>
    
    
      <category term="Linux" scheme="http://yoursite.com/tags/Linux/"/>
    
  </entry>
  
</feed>
